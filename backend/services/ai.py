"""
AI Service
AI å¯¹è¯æœåŠ¡
"""
from typing import List, Optional, AsyncGenerator, Union
from sqlalchemy.ext.asyncio import AsyncSession
from loguru import logger
import asyncio

from core.config import settings
from schemas.ai import ChatMessage
from services.llm_model import LLMModelService
import httpx
import json
import uuid


class AIService:
    """AIå¯¹è¯æœåŠ¡ç±»"""

    def __init__(self, db: AsyncSession):
        self.db = db
        self.llm_model_service = LLMModelService(db)
        # å…¼å®¹æ—§ä»£ç ï¼šå¦‚æœæ²¡æœ‰é…ç½®æ¨¡å‹ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
        self.openai_api_key = settings.OPENAI_API_KEY
        self.openai_base_url = "https://api.deepseek.com"

        # HTTP/2 + Gzip å‹ç¼©çš„å®¢æˆ·ç«¯é…ç½®
        # æ”¯æŒ HTTP/2 ä»¥æå‡æ€§èƒ½ï¼ˆå¤´éƒ¨å‹ç¼©ã€å¤šè·¯å¤ç”¨ï¼‰
        # httpx ä¼šè‡ªåŠ¨å¤„ç† gzip å‹ç¼©ï¼ˆè‡ªåŠ¨æ·»åŠ  Accept-Encoding: gzipï¼‰
        self._client_config = {
            "timeout": httpx.Timeout(120.0, connect=10.0),
            "limits": httpx.Limits(max_keepalive_connections=20, max_connections=100),
            "http2": True,  # å¯ç”¨ HTTP/2 æ”¯æŒ
            "verify": False,  # éªŒè¯ SSL è¯ä¹¦
            "follow_redirects": True,
            "trust_env": False,   # â¬…ï¸ å…³é”®ï¼šç¦ç”¨è¯»å–ç³»ç»Ÿä»£ç†ç¯å¢ƒå˜é‡
        }

    def _update_token_usage_async(self, model_id: str, total_tokens: int) -> None:
        """
        å¼‚æ­¥æ›´æ–° token ä½¿ç”¨ç»Ÿè®¡ï¼ˆåå°ä»»åŠ¡ï¼‰

        ä½¿ç”¨ create_task åˆ›å»ºåå°ä»»åŠ¡ï¼Œä¸é˜»å¡ä¸»æµç¨‹
        """
        async def _do_update():
            # åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
            from db.session import async_session_maker
            async with async_session_maker() as db:
                try:
                    # ä½¿ç”¨æ–°çš„ä¼šè¯åˆ›å»º service
                    llm_service = LLMModelService(db)
                    await llm_service.update_token_usage(model_id, total_tokens)
                    await db.commit()
                except Exception as e:
                    await db.rollback()
                    # åªè®°å½•è­¦å‘Šï¼Œä¸å½±å“ä¸»æµç¨‹
                    logger.warning(f"Failed to update token usage (async): {e}")

        # åˆ›å»ºåå°ä»»åŠ¡
        asyncio.create_task(_do_update())

    
    async def _get_model_config(self, model_id: str) -> tuple[Optional[str], Optional[str]]:
        """
        æ ¹æ®æ¨¡å‹IDè·å–æ¨¡å‹é…ç½®ï¼ˆAPI key å’Œ base_urlï¼‰

        Args:
            model_id: æ¨¡å‹IDï¼ˆå¯èƒ½æ˜¯æ•°æ®åº“IDå­—ç¬¦ä¸²ï¼Œæˆ–æ¨¡å‹æ ‡è¯†å¦‚ "gpt-4o"ï¼‰

        Returns:
            (api_key, base_url) æˆ– (None, None) å¦‚æœæœªæ‰¾åˆ°
        """
        try:
            model = None

            # ä¼˜åŒ–ï¼šç›´æ¥åˆ¤æ–­ç±»å‹ï¼Œé¿å… try-except é€ æˆçš„é‡å¤æŸ¥è¯¢
            if model_id.isdigit():
                # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œä½œä¸ºæ•°æ®åº“IDæŸ¥æ‰¾
                model = await self.llm_model_service.get_llm_model_by_id(int(model_id))
            else:
                # å¦åˆ™ä½œä¸º model_id æŸ¥æ‰¾
                model = await self.llm_model_service.get_llm_model_by_model_id(model_id)

            if model:
                if model.api_key and model.is_enabled:
                    base_url = model.base_url or self.llm_model_service.DEFAULT_BASE_URLS.get(model.provider)
                    # è§„èŒƒåŒ– base_urlï¼šç§»é™¤æœ«å°¾æ–œæ å’Œè·¯å¾„éƒ¨åˆ†
                    if base_url:
                        base_url = base_url.rstrip('/')
                        # ç§»é™¤ /chat/completions ç­‰è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if '/chat/completions' in base_url:
                            base_url = base_url.split('/chat/completions')[0]

                    return model.api_key, base_url
        except Exception as e:
            logger.warning(f"Failed to get model config for {model_id}: {e}")

        return None, None
    
    async def chat(
        self,
        messages: List[Union[dict, ChatMessage]],
        model: str = "gpt-3.5-turbo",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
    ) -> dict:
        """
        éæµå¼å¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹IDï¼ˆæ•°æ®åº“IDå­—ç¬¦ä¸²æˆ–æ¨¡å‹æ ‡è¯†ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            top_p: Top Pé‡‡æ ·
            frequency_penalty: é¢‘ç‡æƒ©ç½š
            presence_penalty: å­˜åœ¨æƒ©ç½š
        
        Returns:
            å¯¹è¯å“åº”å­—å…¸
        """
        # å°è¯•ä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½®
        api_key, base_url = await self._get_model_config(model)
        
        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆå‘åå…¼å®¹ï¼‰
        if not api_key:
            # ä¼˜å…ˆä½¿ç”¨ AI_COLLECT ä¸“ç”¨é…ç½®
            if hasattr(settings, 'AI_COLLECT_API_KEY') and settings.AI_COLLECT_API_KEY:
                api_key = settings.AI_COLLECT_API_KEY
                base_url = settings.AI_COLLECT_BASE_URL or self.openai_base_url
            else:
                # ä½¿ç”¨é€šç”¨é…ç½®
                api_key = self.openai_api_key
                base_url = self.openai_base_url
            
            if not api_key:
                raise ValueError("æ¨¡å‹æœªé…ç½® API Keyï¼Œä¸”ç¯å¢ƒå˜é‡ä¹Ÿæœªé…ç½®ï¼ˆè¯·è®¾ç½® AI_COLLECT_API_KEY æˆ– OPENAI_API_KEYï¼‰")
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼ˆæ”¯æŒChatMessageå¯¹è±¡æˆ–å­—å…¸ï¼‰
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, dict):
                formatted_messages.append(msg)
            else:
                formatted_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # ç¡®å®šå®é™…ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†ï¼ˆä»æ•°æ®åº“æ¨¡å‹é…ç½®ä¸­è·å– model_idï¼‰
        actual_model_id = model
        try:
            try:
                db_id = int(model)
                llm_model = await self.llm_model_service.get_llm_model_by_id(db_id)
                if llm_model:
                    actual_model_id = llm_model.model_id
            except (ValueError, Exception):
                # å¦‚æœ model ä¸æ˜¯æ•°å­—IDï¼Œç›´æ¥ä½¿ç”¨
                pass
        except Exception:
            pass
        
        # è§„èŒƒåŒ– base_url å¹¶æ„å»ºå®Œæ•´ URL
        normalized_base_url = base_url.rstrip('/')
        if '/chat/completions' in normalized_base_url:
            normalized_base_url = normalized_base_url.split('/chat/completions')[0]
        # # æ£€æŸ¥ base_url æ˜¯å¦å·²ç»åŒ…å« /v1ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
        # if '/v1' not in normalized_base_url and not normalized_base_url.endswith('/v1'):
        #     api_url = f"{normalized_base_url}/v1/chat/completions"
        else:
            api_url = f"{normalized_base_url}/chat/completions"

        # æ„å»ºè¯·æ±‚ä½“
        request_body = {
            "model": actual_model_id,
            "messages": formatted_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "frequency_penalty": frequency_penalty,
            "presence_penalty": presence_penalty,
            "stream": False,
        }

        # æ‰‹åŠ¨åºåˆ—åŒ–JSON,ç¡®ä¿æ­£ç¡®çš„ç¼–ç 
        request_body_json = json.dumps(request_body, ensure_ascii=False)
        request_body_size = len(request_body_json.encode('utf-8'))

        # æ„å»ºè¯·æ±‚å¤´
        # httpx ä¼šè‡ªåŠ¨æ·»åŠ  Accept-Encoding: gzip, deflate
        # æ˜¾å¼æ·»åŠ å¯ä»¥ç¡®ä¿å‹ç¼©è¢«å¯ç”¨
        request_headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json; charset=utf-8",
            "Accept-Encoding": "gzip, deflate",  # æ˜¾å¼å¯ç”¨å‹ç¼©
            "X-My-Gate-Key": "Huoyuan2026",  # ç½‘å…³è®¤è¯å¯†é’¥
            "Content-Length": str(request_body_size),
        }

        # è°ƒç”¨ APIï¼ˆä½¿ç”¨ HTTP/2 å’Œ Gzip æ”¯æŒï¼‰
        async with httpx.AsyncClient(**self._client_config) as client:
            response = await client.post(
                api_url,
                headers=request_headers,
                content=request_body_json.encode('utf-8'),  # æ‰‹åŠ¨ç¼–ç ,ä½¿ç”¨contentè€Œä¸æ˜¯json
            )
            
            if response.status_code != 200:
                error_text = response.text

                # ğŸ” è¯¦ç»†é”™è¯¯æ—¥å¿—
                # æŸ¥æ‰¾system prompté•¿åº¦(å¯èƒ½åœ¨ä»»ä½•ä½ç½®)
                system_prompt_length = 'N/A'
                for msg in formatted_messages:
                    if msg.get('role') == 'system':
                        system_prompt_length = len(msg.get('content', ''))
                        break

                    
                logger.error(f"âŒ [API] LLM APIè¯·æ±‚å¤±è´¥ (éæµå¼):")
                logger.error(f"  - HTTP Status: {response.status_code}")
                logger.error(f"  - API URL: {api_url}")
                logger.error(f"  - Model ID: {actual_model_id}")
                logger.error(f"  - Model Type: {model}")
                logger.error(f"  - Response Headers: {dict(response.headers)}")
                logger.error(f"  - Error Response: {error_text[:1000]}")  # é™åˆ¶é•¿åº¦
                logger.error(f"  - Request Messages Count: {len(formatted_messages)}")
                logger.error(f"  - System Prompt Length: {system_prompt_length}")

                # å¦‚æœæ˜¯503,æä¾›ç‰¹åˆ«æç¤º
                if response.status_code == 503:
                    logger.error(f"  âš ï¸ 503é”™è¯¯å¯èƒ½åŸå› :")
                    logger.error(f"    1. APIç½‘å…³è¿‡è½½æˆ–ä¸å¯ç”¨")
                    logger.error(f"    2. Base URLé…ç½®é”™è¯¯: {api_url}")
                    logger.error(f"    3. ç½‘å…³è®¤è¯å¯†é’¥(X-My-Gate-Key)æ— æ•ˆ")
                    logger.error(f"    4. å¤–éƒ¨APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
                    logger.error(f"    ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®åº“ä¸­çš„base_urlå’Œapi_keyé…ç½®")

                raise Exception(f"API è¯·æ±‚å¤±è´¥ (HTTP {response.status_code}): {error_text[:200]}")
            
            data = response.json()

            # æ›´æ–° token ä½¿ç”¨ç»Ÿè®¡ï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼Œä¸é˜»å¡å“åº”ï¼‰
            usage = data.get("usage")
            if usage and isinstance(usage, dict):
                total_tokens = usage.get("total_tokens", 0)
                if total_tokens > 0:
                    # ä½¿ç”¨å¼‚æ­¥åå°ä»»åŠ¡æ›´æ–°ï¼Œä¸é˜»å¡ä¸»æµç¨‹
                    self._update_token_usage_async(actual_model_id, total_tokens)

            # æ ¼å¼åŒ–å“åº”
            return {
                "id": data.get("id", str(uuid.uuid4())),
                "model": data.get("model", actual_model_id),
                "message": {
                    "role": data["choices"][0]["message"]["role"],
                    "content": data["choices"][0]["message"]["content"],
                },
                "usage": usage,
                "finish_reason": data["choices"][0].get("finish_reason"),
            }
    
    async def stream_chat(
        self,
        messages: List[Union[dict, ChatMessage]],
        model: str = "gpt-3.5-turbo",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
    ) -> AsyncGenerator[str, None]:
        """
        SSE æµå¼å¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹IDï¼ˆæ•°æ®åº“IDå­—ç¬¦ä¸²æˆ–æ¨¡å‹æ ‡è¯†ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            top_p: Top Pé‡‡æ ·
            frequency_penalty: é¢‘ç‡æƒ©ç½š
            presence_penalty: å­˜åœ¨æƒ©ç½š
        
        Yields:
            JSON å­—ç¬¦ä¸²æ ¼å¼çš„æµå¼æ•°æ®å—
        """
        # å°è¯•ä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½®
        api_key, base_url = await self._get_model_config(model)
        
        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆå‘åå…¼å®¹ï¼‰
        if not api_key:
            # ä¼˜å…ˆä½¿ç”¨ AI_COLLECT ä¸“ç”¨é…ç½®
            if hasattr(settings, 'AI_COLLECT_API_KEY') and settings.AI_COLLECT_API_KEY:
                api_key = settings.AI_COLLECT_API_KEY
                base_url = settings.AI_COLLECT_BASE_URL or self.openai_base_url
            else:
                # ä½¿ç”¨é€šç”¨é…ç½®
                api_key = self.openai_api_key
                base_url = self.openai_base_url
            
            if not api_key:
                raise ValueError("æ¨¡å‹æœªé…ç½® API Keyï¼Œä¸”ç¯å¢ƒå˜é‡ä¹Ÿæœªé…ç½®ï¼ˆè¯·è®¾ç½® AI_COLLECT_API_KEY æˆ– OPENAI_API_KEYï¼‰")
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼ˆæ”¯æŒChatMessageå¯¹è±¡æˆ–å­—å…¸ï¼‰
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, dict):
                formatted_messages.append(msg)
            else:
                formatted_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # ç¡®å®šå®é™…ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†ï¼ˆä»æ•°æ®åº“æ¨¡å‹é…ç½®ä¸­è·å– model_idï¼‰
        actual_model_id = model
        try:
            try:
                db_id = int(model)
                llm_model = await self.llm_model_service.get_llm_model_by_id(db_id)
                if llm_model:
                    actual_model_id = llm_model.model_id
            except (ValueError, Exception):
                # å¦‚æœ model ä¸æ˜¯æ•°å­—IDï¼Œç›´æ¥ä½¿ç”¨
                pass
        except Exception:
            pass
        
        # è°ƒç”¨ APIï¼ˆæµå¼ï¼‰
        usage_info = None

        # æ„å»ºè¯·æ±‚å¤´
        # httpx ä¼šè‡ªåŠ¨æ·»åŠ  Accept-Encoding: gzip, deflate
        # æ˜¾å¼æ·»åŠ å¯ä»¥ç¡®ä¿å‹ç¼©è¢«å¯ç”¨
        request_headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept-Encoding": "gzip, deflate",  # æ˜¾å¼å¯ç”¨å‹ç¼©
            "X-My-Gate-Key": "Huoyuan2026",  # ç½‘å…³è®¤è¯å¯†é’¥
        }

        # è§„èŒƒåŒ– base_url å¹¶æ„å»ºå®Œæ•´ URL
        normalized_base_url = base_url.rstrip('/')
        if '/chat/completions' in normalized_base_url:
            normalized_base_url = normalized_base_url.split('/chat/completions')[0]
        # æ£€æŸ¥ base_url æ˜¯å¦å·²ç»åŒ…å« /v1ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
        # if '/v1' not in normalized_base_url and not normalized_base_url.endswith('/v1'):
        #     api_url = f"{normalized_base_url}/v1/chat/completions"
        else:
            api_url = f"{normalized_base_url}/chat/completions"

        # ğŸ” è°ƒè¯•æ—¥å¿—: æ‰“å°è¯·æ±‚è¯¦æƒ…
        logger.info(f"ğŸ” [DEBUG] API Request Details:")
        logger.info(f"  - API URL: {api_url}")
        logger.info(f"  - Model: {actual_model_id}")
        logger.info(f"  - Messages count: {len(formatted_messages)}")
        logger.info(f"  - HTTP/2 enabled: True")
        logger.info(f"  - Gzip compression: enabled")
        logger.info(f"  - Request headers keys: {list(request_headers.keys())}")

        # æ‰“å°æ¶ˆæ¯ç»“æ„(ä½†ä¸æ‰“å°å®Œæ•´å†…å®¹,é¿å…æ—¥å¿—è¿‡é•¿)
        for i, msg in enumerate(formatted_messages):
            role = msg.get('role', 'unknown')
            content_len = len(msg.get('content', ''))
            logger.info(f"  - Message {i+1}: role={role}, content_length={content_len}")

        # æ„å»ºè¯·æ±‚ä½“
        request_body = {
            "model": actual_model_id,
            "messages": formatted_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "frequency_penalty": frequency_penalty,
            "presence_penalty": presence_penalty,
            "stream": True,
        }

        # è®¡ç®—å¹¶æ‰“å°è¯·æ±‚ä½“å¤§å°
        # æ‰‹åŠ¨åºåˆ—åŒ–JSON,ä½¿ç”¨ensure_ascii=Falseæ”¯æŒä¸­æ–‡
        request_body_json = json.dumps(request_body, ensure_ascii=False)
        request_body_size = len(request_body_json.encode('utf-8'))
        logger.info(f"  - Request body size: {request_body_size} bytes ({request_body_size/1024:.2f} KB)")
        logger.info(f"  - Estimated compressed size: ~{request_body_size//3} bytes (gzip)")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¯èƒ½å¯¼è‡´é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦
        if request_body_size > 50000:  # 50KB
            logger.warning(f"  âš ï¸ Large request body detected: {request_body_size} bytes")
            logger.warning(f"  This may cause API gateway 503 errors")
            logger.warning(f"  ğŸ’¡ Gzip compression will reduce this by ~70%")

        # ä½¿ç”¨contentå‚æ•°æ‰‹åŠ¨å‘é€JSON,ç¡®ä¿æ­£ç¡®çš„ç¼–ç 
        request_headers["Content-Length"] = str(request_body_size)

        # ä½¿ç”¨ HTTP/2 + Gzip å‹ç¼©çš„å®¢æˆ·ç«¯
        async with httpx.AsyncClient(**self._client_config) as client:
            async with client.stream(
                "POST",
                api_url,
                headers=request_headers,
                content=request_body_json.encode('utf-8'),  # æ‰‹åŠ¨ç¼–ç ,ä½¿ç”¨contentè€Œä¸æ˜¯json
            ) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    error_text_str = error_text.decode('utf-8', errors='ignore') if error_text else ""

                    # ğŸ” è¯¦ç»†é”™è¯¯æ—¥å¿—
                    # æŸ¥æ‰¾system prompté•¿åº¦(å¯èƒ½åœ¨ä»»ä½•ä½ç½®)
                    system_prompt_length = 'N/A'
                    for msg in formatted_messages:
                        if msg.get('role') == 'system':
                            system_prompt_length = len(msg.get('content', ''))
                            break

                    logger.error(f"{msg}")    
                    logger.error(f"âŒ [API] LLM APIè¯·æ±‚å¤±è´¥:")
                    logger.error(f"  - HTTP Status: {response.status_code}")
                    logger.error(f"  - API URL: {api_url}")
                    logger.error(f"  - Model ID: {actual_model_id}")
                    logger.error(f"  - Model Type: {model}")
                    logger.error(f"  - Response Headers: {dict(response.headers)}")
                    logger.error(f"  - Error Response: {error_text_str[:1000]}")  # é™åˆ¶é•¿åº¦
                    logger.error(f"  - Request Messages Count: {len(formatted_messages)}")
                    logger.error(f"  - System Prompt Length: {system_prompt_length}")

                    # å¦‚æœæ˜¯503,æä¾›ç‰¹åˆ«æç¤º
                    if response.status_code == 503:
                        logger.error(f"  âš ï¸ 503é”™è¯¯å¯èƒ½åŸå› :")
                        logger.error(f"    1. APIç½‘å…³è¿‡è½½æˆ–ä¸å¯ç”¨")
                        logger.error(f"    2. Base URLé…ç½®é”™è¯¯: {api_url}")
                        logger.error(f"    3. ç½‘å…³è®¤è¯å¯†é’¥(X-My-Gate-Key)æ— æ•ˆ")
                        logger.error(f"    4. å¤–éƒ¨APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
                        logger.error(f"    ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®åº“ä¸­çš„base_urlå’Œapi_keyé…ç½®")

                    error_chunk = {
                        "error": {
                            "message": f"API è¯·æ±‚å¤±è´¥ (HTTP {response.status_code}): {error_text_str[:200]}",
                            "type": "APIError",
                            "status_code": response.status_code,
                            "api_url": api_url,
                            "model_id": actual_model_id
                        }
                    }
                    yield json.dumps(error_chunk)
                    return
                
                # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹ï¼Œå¦‚æœä¸æ˜¯ SSE æ ¼å¼ï¼Œè¿”å›é”™è¯¯
                content_type = response.headers.get("content-type", "").lower()
                if "text/event-stream" not in content_type and "text/plain" not in content_type and "application/json" not in content_type:
                    # å¯èƒ½æ˜¯ HTML æˆ–å…¶ä»–æ ¼å¼çš„é”™è¯¯å“åº”
                    error_text = await response.aread()
                    error_msg = error_text.decode('utf-8', errors='ignore')[:500]  # é™åˆ¶é•¿åº¦
                    logger.error(f"API returned non-SSE response: {content_type} - {error_msg[:200]}")
                    error_chunk = {
                        "error": {
                            "message": f"API è¿”å›äº†é SSE æ ¼å¼çš„å“åº” (content-type: {content_type})ï¼Œå¯èƒ½æ˜¯è®¤è¯å¤±è´¥æˆ– URL é”™è¯¯",
                            "type": "InvalidResponseError",
                            "details": error_msg[:200] if len(error_msg) > 0 else "æ— é”™è¯¯è¯¦æƒ…"
                        }
                    }
                    yield json.dumps(error_chunk)
                    return
                
                # è§£æ SSE æµ
                buffer = ""
                async for chunk in response.aiter_text():
                    # æ£€æŸ¥ç¬¬ä¸€ä¸ª chunk æ˜¯å¦æ˜¯ HTML å“åº”
                    if chunk.strip().startswith("<!DOCTYPE") or chunk.strip().startswith("<html"):
                        logger.error(f"API returned HTML response instead of SSE stream")
                        error_chunk = {
                            "error": {
                                "message": "API è¿”å›äº† HTML å“åº”è€Œä¸æ˜¯ SSE æµï¼Œå¯èƒ½æ˜¯è®¤è¯å¤±è´¥æˆ– URL é”™è¯¯",
                                "type": "InvalidResponseError",
                                "details": chunk[:200] if len(chunk) > 0 else "æ— é”™è¯¯è¯¦æƒ…"
                            }
                        }
                        yield json.dumps(error_chunk)
                        return
                    
                    buffer += chunk
                    
                    # å¤„ç†å®Œæ•´çš„ SSE æ¶ˆæ¯ï¼ˆä»¥ \n\n åˆ†éš”ï¼‰
                    while "\n\n" in buffer:
                        line, buffer = buffer.split("\n\n", 1)
                        
                        if not line.startswith("data: "):
                            continue
                        
                        data_str = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
                        if data_str.strip() == "[DONE]":
                            # æµç»“æŸæ—¶æ›´æ–° token ä½¿ç”¨ç»Ÿè®¡ï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼Œä¸é˜»å¡å“åº”ï¼‰
                            if usage_info and usage_info.get("total_tokens", 0) > 0:
                                # ä½¿ç”¨å¼‚æ­¥åå°ä»»åŠ¡æ›´æ–°ï¼Œä¸é˜»å¡ä¸»æµç¨‹
                                self._update_token_usage_async(actual_model_id, usage_info["total_tokens"])
                            return
                        
                        try:
                            # è§£æ JSON æ•°æ®
                            data = json.loads(data_str)
                            
                            # ä¿å­˜ usage ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ª chunk ä¸­ï¼‰
                            if "usage" in data:
                                usage_info = data["usage"]
                            
                            # æå– delta content
                            choices = data.get("choices", [])
                            if choices and "delta" in choices[0]:
                                delta = choices[0]["delta"]
                                content = delta.get("content", "")
                                
                                if content:
                                    # æ ¼å¼åŒ–ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
                                    chunk_data = {
                                        "id": data.get("id", ""),
                                        "delta": {
                                            "content": content,
                                            "role": delta.get("role"),
                                        },
                                        "finish_reason": choices[0].get("finish_reason"),
                                    }
                                    yield json.dumps(chunk_data)
                        except json.JSONDecodeError as e:
                            logger.warning(f"Failed to parse SSE data: {data_str[:100]} - {e}")
                            continue

