"""
Client Content Generation Endpoints
Cç«¯å†…å®¹ç”Ÿæˆæ¥å£ï¼ˆå°ç¨‹åº & PCå®˜ç½‘ï¼‰
æ”¯æŒæ™ºèƒ½ä½“åˆ—è¡¨æŸ¥è¯¢ã€å¯¹è¯å¼å†…å®¹ç”Ÿæˆã€å¿«é€Ÿç”Ÿæˆç­‰åŠŸèƒ½
"""
import json
import uuid
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession

from db import get_db
from models.user import User
from core.deps import get_current_miniprogram_user
from services.project import ProjectService
from services.llm_service import LLMFactory
from services.llm_model import LLMModelService
from services.agent import AgentService
from services.conversation import ConversationService
from services.ai import AIService
from services.coin_account import CoinAccountService
from services.coin_calculator import CoinCalculatorService
from middleware.balance_checker import BalanceCheckerMiddleware
from constants.agent import get_agent_config, get_all_agents, AgentType, AGENT_CONFIGS
from utils.response import success
from utils.exceptions import BadRequestException, ServerErrorException, NotFoundException
from loguru import logger
from core.config import settings

router = APIRouter()


# ============== åå°ä»»åŠ¡å‡½æ•° ==============

async def embed_conversation_background_task(
    conversation_id: int,
    user_message_id: int,
    assistant_message_id: int
):
    """
    åå°ä»»åŠ¡ï¼šå‘é‡åŒ–å¯¹è¯ç‰‡æ®µ

    Args:
        conversation_id: ä¼šè¯ID
        user_message_id: ç”¨æˆ·æ¶ˆæ¯ID
        assistant_message_id: AIå›å¤æ¶ˆæ¯ID
    """
    from db.session import async_session_maker
    from services.conversation import ConversationService

    try:
        async with async_session_maker() as db:
            service = ConversationService(db)
            await service.embed_conversation_async(
                conversation_id=conversation_id,
                user_message_id=user_message_id,
                assistant_message_id=assistant_message_id
            )
            logger.info(f"å‘é‡åŒ–å®Œæˆ: ä¼šè¯{conversation_id}, æ¶ˆæ¯{user_message_id}-{assistant_message_id}")
    except Exception as e:
        logger.error(f"å‘é‡åŒ–å¤±è´¥: ä¼šè¯{conversation_id}, é”™è¯¯: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹


async def save_conversation_background_task(
    conversation_id: int,
    user_message: str,
    assistant_message: str,
    user_tokens: int = 0,
    assistant_tokens: int = 0
):
    """
    åå°ä»»åŠ¡ï¼šå°†ä¿å­˜ä»»åŠ¡åŠ å…¥Redisé˜Ÿåˆ—,ç”±é˜Ÿåˆ—Workerå¤„ç†

    Args:
        conversation_id: ä¼šè¯ID
        user_message: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
        assistant_message: AIå›å¤å†…å®¹
        user_tokens: ç”¨æˆ·æ¶ˆæ¯tokenæ•°
        assistant_tokens: AIå›å¤tokenæ•°
    """
    try:
        # ä½¿ç”¨é˜Ÿåˆ—åŒ–å¤„ç†,é¿å…æ•°æ®åº“é”å†²çª
        from db.queue import ConversationQueue

        success = await ConversationQueue.enqueue(
            conversation_id=conversation_id,
            user_message=user_message,
            assistant_message=assistant_message,
            user_tokens=user_tokens,
            assistant_tokens=assistant_tokens
        )

        if success:
            logger.info(
                f"âœ… [åå°ä»»åŠ¡] ä¼šè¯ä¿å­˜ä»»åŠ¡å·²åŠ å…¥é˜Ÿåˆ—: "
                f"ä¼šè¯ID={conversation_id}"
            )
        else:
            # Redisä¸å¯ç”¨æ—¶,é™çº§ä¸ºç›´æ¥ä¿å­˜(ä¿æŒåŸæœ‰é€»è¾‘)
            logger.warning(
                f"âš ï¸ [åå°ä»»åŠ¡] Redisä¸å¯ç”¨,é™çº§ä¸ºç›´æ¥ä¿å­˜: "
                f"ä¼šè¯ID={conversation_id}"
            )
            await save_conversation_fallback(
                conversation_id=conversation_id,
                user_message=user_message,
                assistant_message=assistant_message,
                user_tokens=user_tokens,
                assistant_tokens=assistant_tokens
            )

    except Exception as e:
        logger.error(f"âŒ [åå°ä»»åŠ¡] é˜Ÿåˆ—å…¥é˜Ÿå¤±è´¥: ä¼šè¯{conversation_id}, é”™è¯¯: {e}")
        # é™çº§ä¸ºç›´æ¥ä¿å­˜
        try:
            await save_conversation_fallback(
                conversation_id=conversation_id,
                user_message=user_message,
                assistant_message=assistant_message,
                user_tokens=user_tokens,
                assistant_tokens=assistant_tokens
            )
        except Exception as fallback_error:
            logger.error(
                f"âŒ [åå°ä»»åŠ¡] é™çº§ä¿å­˜ä¹Ÿå¤±è´¥: "
                f"ä¼šè¯{conversation_id}, é”™è¯¯={fallback_error}"
            )


async def save_conversation_fallback(
    conversation_id: int,
    user_message: str,
    assistant_message: str,
    user_tokens: int = 0,
    assistant_tokens: int = 0
):
    """
    é™çº§æ–¹æ¡ˆ: ç›´æ¥ä¿å­˜å¯¹è¯æ¶ˆæ¯(Redisä¸å¯ç”¨æ—¶ä½¿ç”¨)

    Args:
        conversation_id: ä¼šè¯ID
        user_message: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
        assistant_message: AIå›å¤å†…å®¹
        user_tokens: ç”¨æˆ·æ¶ˆæ¯tokenæ•°
        assistant_tokens: AIå›å¤tokenæ•°
    """
    from db.session import async_session_maker
    from services.conversation import ConversationService
    from sqlalchemy import select, desc
    from models.conversation import ConversationMessage

    try:
        # 1. ä¿å­˜å¯¹è¯æ¶ˆæ¯
        async with async_session_maker() as db:
            service = ConversationService(db)
            await service.save_conversation_async(
                conversation_id=conversation_id,
                user_message=user_message,
                assistant_message=assistant_message,
                user_tokens=user_tokens,
                assistant_tokens=assistant_tokens
            )

        # 2. è·å–åˆšä¿å­˜çš„æ¶ˆæ¯IDå¹¶è§¦å‘å‘é‡åŒ–
        async with async_session_maker() as db:
            # æŸ¥è¯¢æœ€æ–°çš„ä¸¤æ¡æ¶ˆæ¯ï¼ˆuser + assistantï¼‰
            query = select(ConversationMessage).where(
                ConversationMessage.conversation_id == conversation_id
            ).order_by(desc(ConversationMessage.sequence)).limit(2)

            result = await db.execute(query)
            messages = list(result.scalars().all())

            if len(messages) == 2:
                # messages[0]æ˜¯assistant, messages[1]æ˜¯userï¼ˆé™åºï¼‰
                assistant_msg = messages[0]
                user_msg = messages[1]

                # è§¦å‘å‘é‡åŒ–ä»»åŠ¡
                await embed_conversation_background_task(
                    conversation_id=conversation_id,
                    user_message_id=user_msg.id,
                    assistant_message_id=assistant_msg.id
                )
            else:
                logger.warning(f"æ— æ³•æ‰¾åˆ°æ¶ˆæ¯è¿›è¡Œå‘é‡åŒ–: ä¼šè¯{conversation_id}")

    except Exception as e:
        logger.error(f"é™çº§ä¿å­˜å¤±è´¥: ä¼šè¯{conversation_id}, é”™è¯¯: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹


# ============== Request/Response Models ==============

class ChatMessage(BaseModel):
    """å¯¹è¯æ¶ˆæ¯æ¨¡å‹"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: 'user' æˆ– 'assistant'")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatRequest(BaseModel):
    """å¯¹è¯å¼åˆ›ä½œè¯·æ±‚æ¨¡å‹"""
    conversation_id: Optional[int] = Field(default=None, description="ä¼šè¯IDï¼ˆå¯é€‰ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°ä¼šè¯ï¼‰")
    project_id: Optional[int] = Field(default=None, description="é¡¹ç›®IDï¼Œç”¨äºè·å–IPäººè®¾ä¿¡æ¯")
    agent_type: str = Field(default=AgentType.EFFICIENT_ORAL, description="æ™ºèƒ½ä½“ç±»å‹")
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯å†å²æ¶ˆæ¯åˆ—è¡¨")
    model_type: Optional[str] = Field(default=None, description="LLMæ¨¡å‹ç±»å‹ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™ä½¿ç”¨æ™ºèƒ½ä½“é…ç½®çš„æ¨¡å‹ï¼‰")
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0, description="ç”Ÿæˆæ¸©åº¦")
    max_tokens: int = Field(default=2048, ge=1, le=8192, description="æœ€å¤§ç”Ÿæˆtokens")
    stream: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º")


class ChatResponse(BaseModel):
    """å¯¹è¯å“åº”æ¨¡å‹ï¼ˆéæµå¼ï¼‰"""
    success: bool = True
    content: str = Field(..., description="ç”Ÿæˆçš„å†…å®¹")
    agent_type: str = Field(..., description="ä½¿ç”¨çš„æ™ºèƒ½ä½“ç±»å‹")
    model_type: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹ç±»å‹")


class AgentInfo(BaseModel):
    """æ™ºèƒ½ä½“ä¿¡æ¯æ¨¡å‹"""
    type: str  # æ™ºèƒ½ä½“ç±»å‹æ ‡è¯†ï¼Œç”¨äºæ˜ å°„åˆ°åç«¯çš„ agent_type
    id: str  # æ™ºèƒ½ä½“IDï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
    name: str
    icon: str
    description: str


class AgentListResponse(BaseModel):
    """æ™ºèƒ½ä½“åˆ—è¡¨å“åº”"""
    success: bool = True
    agents: List[AgentInfo]


# ============== Helper Functions ==============

def build_ip_persona_prompt(project) -> str:
    """ä»é¡¹ç›®ä¿¡æ¯æ„å»ºIPäººè®¾æç¤ºè¯"""
    if not project:
        return ""
    
    persona = project.get_persona_settings_dict()
    parts = []
    
    parts.append(f"ã€IPä¿¡æ¯ã€‘")
    parts.append(f"- IPåç§°ï¼š{project.name}")
    parts.append(f"- æ‰€å±èµ›é“ï¼š{project.industry}")
    
    if persona.get("introduction"):
        parts.append(f"- IPç®€ä»‹ï¼š{persona['introduction']}")
    
    if persona.get("tone"):
        parts.append(f"- è¯­æ°”é£æ ¼ï¼š{persona['tone']}")
    
    if persona.get("target_audience"):
        parts.append(f"- ç›®æ ‡å—ä¼—ï¼š{persona['target_audience']}")
    
    if persona.get("content_style"):
        parts.append(f"- å†…å®¹é£æ ¼ï¼š{persona['content_style']}")
    
    if persona.get("catchphrase"):
        parts.append(f"- å¸¸ç”¨å£å¤´ç¦…ï¼š{persona['catchphrase']}")
    
    if persona.get("keywords"):
        parts.append(f"- å¸¸ç”¨å…³é”®è¯ï¼š{', '.join(persona['keywords'])}")
    
    if persona.get("taboos"):
        parts.append(f"- å†…å®¹ç¦å¿Œï¼š{', '.join(persona['taboos'])}")
    
    if persona.get("benchmark_accounts"):
        parts.append(f"- å¯¹æ ‡è´¦å·ï¼š{', '.join(persona['benchmark_accounts'])}")
    
    return "\n".join(parts)


def build_final_system_prompt(agent_system_prompt: str, ip_persona_prompt: str) -> str:
    """èåˆæ™ºèƒ½ä½“äººè®¾å’ŒIPç”»åƒï¼Œæ„å»ºæœ€ç»ˆçš„System Prompt"""
    parts = [agent_system_prompt]
    
    if ip_persona_prompt:
        parts.append("\n\n" + "=" * 40)
        parts.append("\nåœ¨åˆ›ä½œæ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹IPäººè®¾è®¾å®šï¼Œç¡®ä¿å†…å®¹ç¬¦åˆè¯¥IPçš„é£æ ¼ç‰¹ç‚¹ï¼š\n")
        parts.append(ip_persona_prompt)
        parts.append("\n" + "=" * 40)
        parts.append("\nè¯·åœ¨ä¿æŒæ™ºèƒ½ä½“ä¸“ä¸šèƒ½åŠ›çš„åŒæ—¶ï¼Œèå…¥ä»¥ä¸ŠIPçš„äººè®¾ç‰¹ç‚¹è¿›è¡Œåˆ›ä½œã€‚")
    
    return "".join(parts)


def get_latest_user_message(messages: List[ChatMessage]) -> str:
    """å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºç”¨äºLLMçš„prompt"""
    for msg in reversed(messages):
        if msg.role == "user":
            return msg.content
    
    return messages[-1].content if messages else ""


def build_conversation_context(messages: List[ChatMessage]) -> str:
    """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç”¨äºå¤šè½®å¯¹è¯"""
    if len(messages) <= 1:
        return ""
    
    history = messages[:-1]
    if not history:
        return ""
    
    context_parts = ["\nã€å¯¹è¯å†å²ã€‘"]
    for msg in history[-6:]:
        role_name = "ç”¨æˆ·" if msg.role == "user" else "åŠ©æ‰‹"
        context_parts.append(f"{role_name}ï¼š{msg.content}")
    
    context_parts.append("\nè¯·åŸºäºä»¥ä¸Šå¯¹è¯å†å²ï¼Œç»§ç»­å›å¤ç”¨æˆ·çš„æœ€æ–°è¯·æ±‚ã€‚")
    
    return "\n".join(context_parts)


# ============== API Endpoints ==============

@router.get("/agents")
async def list_agents(
    db: AsyncSession = Depends(get_db)
):
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ™ºèƒ½ä½“åˆ—è¡¨ï¼ˆä»æ•°æ®åº“è¯»å–ï¼‰"""
    # ä»æ•°æ®åº“æŸ¥è¯¢å¯ç”¨çš„æ™ºèƒ½ä½“
    from sqlalchemy import select, and_
    from models.agent import Agent
    
    result = await db.execute(
        select(Agent).where(
            and_(
                Agent.status == 1,  # åªè¿”å›ä¸Šæ¶çš„æ™ºèƒ½ä½“
                Agent.is_system == 0  # è¿‡æ»¤æ‰ç³»ç»Ÿè‡ªç”¨æ™ºèƒ½ä½“
            )
        ).order_by(Agent.sort_order, Agent.created_at)
    )
    db_agents = result.scalars().all()
    
    # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼ï¼Œç¡®ä¿ id ä¸º number ç±»å‹
    agents = []
    for agent in db_agents:
        # å¦‚æœæ•°æ®åº“ä¸­æœ‰ type å­—æ®µï¼Œä½¿ç”¨ agent.typeï¼›å¦åˆ™ä½¿ç”¨ agent.id
        agent_type = str(agent.id)  # æš‚æ—¶ä½¿ç”¨ ID ä½œä¸º type
        
        # å°è¯•ä» config ä¸­è·å– typeï¼ˆå¦‚æœä¹‹å‰æœ‰å­˜å‚¨ï¼‰
        if agent.config and isinstance(agent.config, dict) and "type" in agent.config:
            agent_type = agent.config["type"]
        
        agents.append({
            "type": agent_type,
            "id": agent.id,  # ç»Ÿä¸€ä¸º number ç±»å‹
            "name": agent.name,
            "icon": agent.icon,
            "description": agent.description or ""
        })
    
    return success(data={"agents": agents}, msg="è·å–æˆåŠŸ")


@router.post("/chat")
async def generate_chat(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_miniprogram_user),
    db: AsyncSession = Depends(get_db)
):
    """å¯¹è¯å¼åˆ›ä½œæ¥å£ï¼ˆæ”¯æŒå‘é‡æ£€ç´¢å’Œå¼‚æ­¥ä¿å­˜ï¼‰"""
    try:
        # 0. åˆå§‹åŒ–ä¼šè¯æœåŠ¡
        conversation_service = ConversationService(db)

        # 0.1. è·å–æ™ºèƒ½ä½“é…ç½®å’Œæ¨¡å‹ç±»å‹ï¼ˆæå‰è·å–ï¼Œé¿å…é‡å¤æŸ¥è¯¢ï¼‰
        agent_config = None
        agent_type_source = "preset"
        db_agent = None
        agent_model_type = request.model_type or "doubao"  # é»˜è®¤å€¼

        try:
            agent_config = get_agent_config(request.agent_type)
        except ValueError:
            # å¦‚æœé¢„è®¾é…ç½®ä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¯èƒ½æ˜¯æ•°æ®åº“IDï¼‰
            try:
                agent_id = int(request.agent_type)
                from sqlalchemy import select
                from models.agent import Agent

                result = await db.execute(
                    select(Agent).where(
                        Agent.id == agent_id,
                        Agent.status == 1  # åªæŸ¥è¯¢ä¸Šæ¶çš„æ™ºèƒ½ä½“
                    )
                )
                db_agent = result.scalar_one_or_none()

                if db_agent:
                    # ä»æ•°æ®åº“æ™ºèƒ½ä½“æ„å»ºé…ç½®
                    agent_config = {
                        "system_prompt": db_agent.system_prompt,
                        "temperature": db_agent.config.get("temperature", 0.7) if db_agent.config else 0.7,
                        "max_tokens": db_agent.config.get("max_tokens", 2048) if db_agent.config else 2048,
                    }
                    agent_type_source = "database"
                    # ä½¿ç”¨æ•°æ®åº“æ™ºèƒ½ä½“é…ç½®çš„æ¨¡å‹
                    agent_model_type = db_agent.model
                    logger.info(f"ğŸ“Š [DEBUG] ä½¿ç”¨æ•°æ®åº“æ™ºèƒ½ä½“é…ç½®çš„æ¨¡å‹: {agent_model_type}")
                else:
                    available = ", ".join(AGENT_CONFIGS.keys())
                    raise BadRequestException(f"æ™ºèƒ½ä½“ ID '{agent_id}' ä¸å­˜åœ¨æˆ–å·²ä¸‹æ¶ã€‚å¯ç”¨ç±»å‹: {available}")
            except ValueError:
                # agent_type ä¸æ˜¯æ•°å­—ï¼Œä¹Ÿä¸æ˜¯é¢„è®¾æšä¸¾å€¼
                available = ", ".join(AGENT_CONFIGS.keys())
                raise BadRequestException(f"æœªçŸ¥çš„æ™ºèƒ½ä½“ç±»å‹: '{request.agent_type}'ã€‚å¯ç”¨ç±»å‹: {available}")

        if not agent_config:
            available = ", ".join(AGENT_CONFIGS.keys())
            raise BadRequestException(f"æ— æ³•è·å–æ™ºèƒ½ä½“é…ç½®: '{request.agent_type}'ã€‚å¯ç”¨ç±»å‹: {available}")

        # å¦‚æœæ˜¯é¢„è®¾æ™ºèƒ½ä½“ä¸”æ²¡æœ‰æä¾›model_typeï¼Œä½¿ç”¨é»˜è®¤å€¼
        if agent_type_source == "preset" and not request.model_type:
            agent_model_type = "doubao"
            logger.info(f"ğŸ“Š [DEBUG] ä½¿ç”¨é»˜è®¤æ¨¡å‹: {agent_model_type}")

        # 0.2. ä¸å†éªŒè¯æ¨¡å‹ç±»å‹ï¼Œæ‰€æœ‰æ¨¡å‹ä¿¡æ¯ä»æ•°æ®åº“è¯»å–
        # è¿™æ ·å¯ä»¥æ”¯æŒåŠ¨æ€æ·»åŠ æ–°æ¨¡å‹ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
        logger.info(f"ğŸ“Š [DEBUG] ä½¿ç”¨æ¨¡å‹ç±»å‹: {agent_model_type} (æ¥æº: {agent_type_source})")

        # 0.1. å¤„ç†ä¼šè¯IDï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°ä¼šè¯ï¼‰
        conversation_id = request.conversation_id
        if not conversation_id:
            from schemas.conversation import ConversationCreate
            from sqlalchemy import select
            from models.agent import Agent
            
            # è·å–æ™ºèƒ½ä½“åç§°ç”¨äºç”Ÿæˆä¼šè¯æ ‡é¢˜
            agent_name = "æ–°å¯¹è¯"
            agent_id = None
            if request.agent_type.isdigit():
                agent_id = int(request.agent_type)
                result = await db.execute(
                    select(Agent).where(Agent.id == agent_id)
                )
                db_agent = result.scalar_one_or_none()
                if db_agent:
                    agent_name = db_agent.name
            
            # è·å–ç”¨æˆ·çš„ç¬¬ä¸€å¥è¯ï¼ˆæˆªå–å‰30ä¸ªå­—ç¬¦ï¼‰
            first_message = ""
            for msg in request.messages:
                if msg.role == "user" and msg.content:
                    first_message = msg.content[:30]
                    if len(msg.content) > 30:
                        first_message += "..."
                    break
            
            # ç”Ÿæˆä¼šè¯æ ‡é¢˜ï¼šæ™ºèƒ½ä½“åç§° + ç”¨æˆ·ç¬¬ä¸€å¥è¯
            title = f"{agent_name}: {first_message}" if first_message else agent_name
            
            conversation_data = ConversationCreate(
                agent_id=agent_id,
                project_id=request.project_id,
                model_type=agent_model_type,
                title=title,
            )
            conversation = await conversation_service.create_conversation(
                user_id=current_user.id,
                conversation_data=conversation_data
            )
            conversation_id = conversation.id
        else:
            # éªŒè¯ä¼šè¯æ˜¯å¦å±äºå½“å‰ç”¨æˆ·
            try:
                await conversation_service.get_conversation_by_id(
                    conversation_id=conversation_id,
                    user_id=current_user.id
                )
            except NotFoundException:
                # å¦‚æœä¼šè¯ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ä¼šè¯ï¼ˆå¯èƒ½æ˜¯å‰ç«¯å­˜å‚¨äº†å·²åˆ é™¤çš„ä¼šè¯IDï¼‰
                from schemas.conversation import ConversationCreate
                from sqlalchemy import select
                from models.agent import Agent

                logger.warning(f"ä¼šè¯ {conversation_id} ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»ºæ–°ä¼šè¯ï¼ˆç”¨æˆ·ID: {current_user.id}ï¼‰")
                
                # è·å–æ™ºèƒ½ä½“åç§°ç”¨äºç”Ÿæˆä¼šè¯æ ‡é¢˜
                agent_name = "æ–°å¯¹è¯"
                agent_id = None
                if request.agent_type.isdigit():
                    agent_id = int(request.agent_type)
                    result = await db.execute(
                        select(Agent).where(Agent.id == agent_id)
                    )
                    db_agent = result.scalar_one_or_none()
                    if db_agent:
                        agent_name = db_agent.name
                
                # è·å–ç”¨æˆ·çš„ç¬¬ä¸€å¥è¯ï¼ˆæˆªå–å‰30ä¸ªå­—ç¬¦ï¼‰
                first_message = ""
                for msg in request.messages:
                    if msg.role == "user" and msg.content:
                        first_message = msg.content[:30]
                        if len(msg.content) > 30:
                            first_message += "..."
                        break
                
                # ç”Ÿæˆä¼šè¯æ ‡é¢˜ï¼šæ™ºèƒ½ä½“åç§° + ç”¨æˆ·ç¬¬ä¸€å¥è¯
                title = f"{agent_name}: {first_message}" if first_message else agent_name

                conversation_data = ConversationCreate(
                    agent_id=agent_id,
                    project_id=request.project_id,
                    model_type=agent_model_type,
                    title=title,
                )
                conversation = await conversation_service.create_conversation(
                    user_id=current_user.id,
                    conversation_data=conversation_data
                )
                conversation_id = conversation.id
            except Exception as e:
                # å…¶ä»–é”™è¯¯æ­£å¸¸æŠ›å‡º
                raise

        # 1. è·å–é¡¹ç›®IPç”»åƒï¼ˆå¦‚æœæä¾›äº†project_idï¼‰
        ip_persona_prompt = ""
        if request.project_id:
            project_service = ProjectService(db)
            project = await project_service.get_project_by_id(request.project_id, user_id=current_user.id)
            if project:
                ip_persona_prompt = build_ip_persona_prompt(project)

        # 2. è·å–ç”¨æˆ·æœ€æ–°æ¶ˆæ¯ä½œä¸ºprompt
        user_prompt = get_latest_user_message(request.messages)
        
        if not user_prompt:
            raise BadRequestException("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # 5. å‘é‡æ£€ç´¢ï¼šæœç´¢ç›¸å…³å†å²ç‰‡æ®µ
        # âš ï¸ ä¸´æ—¶ç¦ç”¨å‘é‡æ£€ç´¢ä»¥æå‡æ€§èƒ½ï¼ˆå¾…åç«¯å‘é‡åŒ–åŠŸèƒ½ä¿®å¤åé‡æ–°å¯ç”¨ï¼‰
        # TODO: ä¿®å¤å‘é‡åŒ–åå°ä»»åŠ¡åé‡æ–°å¯ç”¨æ­¤åŠŸèƒ½
        relevant_chunks = []
        optimized_messages = request.messages  # é»˜è®¤ä½¿ç”¨åŸå§‹æ¶ˆæ¯

        # ç¦ç”¨å‘é‡æ£€ç´¢ä»£ç  - èŠ‚çœ200-500mså“åº”æ—¶é—´
        # try:
        #     # å¯¹ç”¨æˆ·æ–°æ¶ˆæ¯è¿›è¡Œå‘é‡åŒ–å¹¶æœç´¢ç›¸å…³ç‰‡æ®µ
        #     relevant_chunks = await conversation_service.search_relevant_chunks(
        #         conversation_id=conversation_id,
        #         query_text=user_prompt,
        #         top_k=5,
        #         threshold=0.7
        #     )
        #
        #     # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³ç‰‡æ®µï¼Œä½¿ç”¨ä¼˜åŒ–çš„æ¶ˆæ¯ä¸Šä¸‹æ–‡
        #     if relevant_chunks:
        #         optimized_messages = await conversation_service.build_context_from_search(
        #             conversation_id=conversation_id,
        #             query_text=user_prompt,
        #             relevant_chunks=relevant_chunks,
        #             include_recent=2  # åŒ…å«æœ€è¿‘2è½®å¯¹è¯
        #         )
        # except Exception as e:
        #     # å‘é‡æ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ¶ˆæ¯
        #     from loguru import logger
        #     logger.warning(f"å‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯: {e}")
        
        # 6. æ„å»ºæœ€ç»ˆSystem Promptï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„æ¶ˆæ¯æˆ–åŸå§‹æ¶ˆæ¯æ„å»ºä¸Šä¸‹æ–‡ï¼‰
        base_system_prompt = agent_config["system_prompt"]
        
        # å¦‚æœæœ‰ä¼˜åŒ–çš„æ¶ˆæ¯ï¼Œæ„å»ºä¸Šä¸‹æ–‡
        if relevant_chunks:
            # ä½¿ç”¨ä¼˜åŒ–åçš„æ¶ˆæ¯æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåªåŒ…å«ç›¸å…³ç‰‡æ®µå’Œæœ€è¿‘æ¶ˆæ¯ï¼‰
            conversation_context = ""
        else:
            # å›é€€åˆ°åŸå§‹é€»è¾‘ï¼šä½¿ç”¨å…¨éƒ¨æ¶ˆæ¯æ„å»ºä¸Šä¸‹æ–‡
            conversation_context = build_conversation_context(optimized_messages)
        
        final_system_prompt = build_final_system_prompt(
            agent_system_prompt=base_system_prompt + conversation_context,
            ip_persona_prompt=ip_persona_prompt,
        )

        # ğŸ” æ£€æŸ¥å¹¶é™åˆ¶ system prompt é•¿åº¦
        MAX_SYSTEM_PROMPT_LENGTH = 8000  # æ ¹æ®æ¨¡å‹é™åˆ¶è°ƒæ•´
        original_length = len(final_system_prompt)
        if original_length > MAX_SYSTEM_PROMPT_LENGTH:
            logger.warning(f"âš ï¸ [DEBUG] System prompt too long ({original_length} chars), truncating to {MAX_SYSTEM_PROMPT_LENGTH} chars")
            logger.warning(f"  - Base agent prompt length: {len(base_system_prompt)} chars")
            logger.warning(f"  - Conversation context length: {len(conversation_context)} chars")
            logger.warning(f"  - IP persona prompt length: {len(ip_persona_prompt)} chars")

            # æ™ºèƒ½æˆªæ–­ç­–ç•¥: ä¿ç•™æ™ºèƒ½ä½“æ ¸å¿ƒprompt,ç²¾ç®€å†å²å’ŒIPä¿¡æ¯
            # 1. å…ˆä¿ç•™å®Œæ•´çš„æ™ºèƒ½ä½“prompt
            truncated_prompt = base_system_prompt

            # 2. å¦‚æœè¿˜æœ‰ç©ºé—´,æ·»åŠ IPäººè®¾çš„å…³é”®éƒ¨åˆ†
            remaining_space = MAX_SYSTEM_PROMPT_LENGTH - len(truncated_prompt) - 100  # ç•™100å­—ç¬¦buffer
            if remaining_space > 0 and ip_persona_prompt:
                # åªä¿ç•™IPäººè®¾çš„å‰Nä¸ªå­—ç¬¦
                ip_persona_truncated = ip_persona_prompt[:remaining_space]
                truncated_prompt = build_final_system_prompt(
                    agent_system_prompt=truncated_prompt,
                    ip_persona_prompt=ip_persona_truncated
                )

            # 3. å¦‚æœè¿˜æ²¡åˆ°é™åˆ¶,æ·»åŠ å¯¹è¯å†å²(æœ€å¤š2è½®)
            if len(truncated_prompt) < MAX_SYSTEM_PROMPT_LENGTH * 0.8 and conversation_context:
                # ç®€åŒ–å¯¹è¯å†å²:åªä¿ç•™æœ€è¿‘2è½®
                simplified_context = "\nã€æœ€è¿‘å¯¹è¯ã€‘" + "\n".join(conversation_context.split("\n")[-6:])
                final_system_prompt = truncated_prompt + "\n\n" + simplified_context
            else:
                final_system_prompt = truncated_prompt

            logger.warning(f"  - After truncation: {len(final_system_prompt)} chars")
        else:
            logger.info(f"âœ… [DEBUG] System prompt length OK: {original_length} chars")

        # å¦‚æœä½¿ç”¨ä¼˜åŒ–åçš„æ¶ˆæ¯ï¼Œéœ€è¦é‡æ–°æ ¼å¼åŒ–user_prompt
        if relevant_chunks:
            # ä»ä¼˜åŒ–çš„æ¶ˆæ¯ä¸­æå–ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€åä¸€æ¡useræ¶ˆæ¯ï¼‰
            user_prompt = user_prompt  # ä¿æŒåŸæ ·ï¼Œå› ä¸ºå·²ç»åœ¨optimized_messagesçš„æœ€å
        
        # 7. ç¡®å®šç”Ÿæˆå‚æ•°
        temperature = request.temperature if request.temperature is not None else agent_config.get("temperature", 0.7)
        max_tokens = request.max_tokens or agent_config.get("max_tokens", 2048)
        
        # 8. ä»ï¿½ï¿½æ®åº“è·å–æ¨¡å‹é…ç½®
        # ç›´æ¥é€šè¿‡ model_type (æˆ– model_id) æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ¨¡å‹é…ç½®
        # æ”¯æŒä¸¤ç§æŸ¥è¯¢æ–¹å¼:
        # 1. é€šè¿‡ provider å­—æ®µæŸ¥è¯¢ (å…¼å®¹æ—§çš„ model_type å¦‚ "deepseek", "doubao")
        # 2. é€šè¿‡ model_id å­—æ®µæŸ¥è¯¢ (æ”¯æŒæ•°æ®åº“ä¸­å­˜å‚¨çš„æ¨¡å‹ID)
        from sqlalchemy import select, and_, or_
        from models.llm_model import LLMModel

        logger.info(f"ğŸ” [DEBUG] Querying model configuration:")
        logger.info(f"  - Requested model_type: {agent_model_type}")

        # å°è¯•é€šè¿‡ provider æˆ– model_id æŸ¥è¯¢
        result = await db.execute(
            select(LLMModel).where(
                and_(
                    or_(
                        LLMModel.provider == agent_model_type.lower(),
                        LLMModel.model_id == agent_model_type,
                        LLMModel.id == int(agent_model_type) if agent_model_type.isdigit() else False
                    ),
                    LLMModel.is_enabled == True
                )
            ).order_by(LLMModel.sort_order).limit(1)
        )
        llm_model = result.scalar_one_or_none()

        if not llm_model:
            # ğŸ” è¯¦ç»†é”™è¯¯æ—¥å¿—: æŸ¥è¯¢å¤±è´¥çš„åŸå› 
            logger.error(f"âŒ [DEBUG] Model not found in database:")
            logger.error(f"  - Requested model_type: {agent_model_type}")

            # æŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹,å¸®åŠ©è°ƒè¯•
            all_enabled = await db.execute(
                select(LLMModel).where(LLMModel.is_enabled == True)
            )
            enabled_models = all_enabled.scalars().all()
            logger.error(f"  - All enabled models in database:")
            for m in enabled_models:
                logger.error(f"    * {m.name} (id={m.id}, provider={m.provider}, model_id={m.model_id}, enabled={m.is_enabled})")

            raise BadRequestException(
                f"æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ '{agent_model_type}'ï¼Œè¯·åœ¨ç®¡ç†åå°é…ç½®æ¨¡å‹"
            )

        logger.info(f"âœ… [DEBUG] Model found: {llm_model.name} (id={llm_model.id}, provider={llm_model.provider})")

        if not llm_model.api_key:
            raise BadRequestException(f"æ¨¡å‹ {llm_model.name} æœªé…ç½® API Keyï¼Œè¯·åœ¨ç®¡ç†åå°é…ç½®")

        # ========== âœ… ç¬¬ä¸€é˜¶æ®µï¼šç®—åŠ›é¢„å†»ç»“ï¼ˆæçŸ­äº‹åŠ¡ï¼Œ~10msï¼‰ ==========
        task_id = str(uuid.uuid4())
        request_id = f"chat_{current_user.id}_{task_id}"  # âœ… å¹‚ç­‰æ€§request_id
        balance_checker = BalanceCheckerMiddleware(db)
        account_service = balance_checker.account_service
        estimated_output_tokens = request.max_tokens or 2048

        try:
            # 1ï¸âƒ£ å…ˆè®¡ç®—é¢„ä¼°æˆæœ¬ï¼ˆä¸æ¶‰åŠæ•°æ®åº“æ“ä½œï¼‰
            user_input_text = user_prompt

            calculator = CoinCalculatorService(db)
            estimated_cost = await calculator.estimate_max_cost(
                model_id=llm_model.id,
                input_text=user_input_text,
                estimated_output_tokens=estimated_output_tokens
            )

            logger.info(
                f"ğŸ’° [åŸå­å†»ç»“] é¢„ä¼°æˆæœ¬: "
                f"ç”¨æˆ·ID={current_user.id}, "
                f"é¢„ä¼°é‡‘é¢={estimated_cost}, "
                f"request_id={request_id}"
            )

            # 2ï¸âƒ£ ä½¿ç”¨åŸå­åŒ–å†»ç»“ï¼ˆæ— é”å†²çªï¼‰
            freeze_result = await account_service.freeze_amount_atomic(
                user_id=current_user.id,
                amount=estimated_cost,  # âœ… ä½¿ç”¨é¢„ä¼°æˆæœ¬
                request_id=request_id,
                model_id=llm_model.id,
                conversation_id=conversation_id
            )

            if not freeze_result['success']:
                if freeze_result.get('insufficient_balance'):
                    # ä½™é¢ä¸è¶³ï¼Œç›´æ¥è¿”å›é”™è¯¯
                    logger.warning(f"âŒ [åŸå­å†»ç»“] ç”¨æˆ·ä½™é¢ä¸è¶³: ç”¨æˆ·ID={current_user.id}")
                    raise BadRequestException(
                        f"ä½™é¢ä¸è¶³ã€‚é¢„ä¼°éœ€è¦: {estimated_cost:.4f} ç«æºå¸ï¼Œè¯·å……å€¼åå†è¯•ã€‚"
                    )
                else:
                    # å…¶ä»–é”™è¯¯
                    logger.error(f"âŒ [åŸå­å†»ç»“] å†»ç»“å¤±è´¥: ç”¨æˆ·ID={current_user.id}")
                    raise BadRequestException("ç®—åŠ›å†»ç»“å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")

            freeze_info = {
                'task_id': task_id,
                'request_id': request_id,
                'freeze_log_id': freeze_result['freeze_log_id'],
                'frozen_amount': estimated_cost,  # âœ… ä¿å­˜é¢„ä¼°é‡‘é¢
                'estimated_cost': estimated_cost
            }

            logger.info(
                f"âœ… [åŸå­å†»ç»“] ç®—åŠ›é¢„å†»ç»“æˆåŠŸ: "
                f"ç”¨æˆ·ID={current_user.id}, "
                f"request_id={request_id}, "
                f"å†»ç»“é‡‘é¢={estimated_cost}, "
                f"å†»ç»“è®°å½•ID={freeze_result['freeze_log_id']}"
            )

        except BadRequestException:
            # ä½™é¢ä¸è¶³ï¼Œç›´æ¥è¿”å›é”™è¯¯
            raise
        except Exception as e:
            # é¢„å†»ç»“å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸é˜»æ­¢è¯·æ±‚ï¼ˆé™çº§å¤„ç†ï¼‰
            logger.warning(f"âš ï¸ [DEBUG] ç®—åŠ›é¢„å†»ç»“å¤±è´¥ï¼ˆé™çº§å¤„ç†ï¼‰: {str(e)}")
            task_id = None  # æ ‡è®°ä¸ºæœªé¢„å†»ç»“ï¼Œè·³è¿‡ç»“ç®—

        # 9. æ„å»º messages åˆ—è¡¨ï¼ˆä¸ AIService å…¼å®¹çš„æ ¼å¼ï¼‰
        # å°† final_system_prompt å’Œ user_prompt è½¬æ¢ä¸º messages æ ¼å¼
        messages_for_ai = []

        # ğŸ” æ™ºèƒ½å¤„ç†é•¿system prompt: å°†é•¿æç¤ºè¯æ‹†åˆ†æˆå¤šä¸ªmessage,é¿å…ç½‘å…³503é”™è¯¯
        # ç­–ç•¥:
        # 1. çŸ­æç¤ºè¯(<3000 chars): ä½¿ç”¨å•ä¸ª system message
        # 2. é•¿æç¤ºè¯(>=3000 chars): æ‹†åˆ†æˆå¤šä¸ª system message,æ¯ä¸ª<2000 chars
        # 3. è¿™æ ·æ—¢ä¿ç•™å®Œæ•´ä¿¡æ¯,åˆé¿å…å•ä¸ªmessageè¿‡é•¿å¯¼è‡´ç½‘å…³æ‹’ç»

        MAX_SINGLE_MESSAGE_LENGTH = 2000  # å•ä¸ªmessageçš„æœ€å¤§é•¿åº¦
        USE_SPLIT_STRATEGY = len(final_system_prompt) >= MAX_SINGLE_MESSAGE_LENGTH

        if USE_SPLIT_STRATEGY:
            # é•¿æç¤ºè¯: æ‹†åˆ†æˆå¤šä¸ª system message
            logger.info(f"ğŸ“Š [DEBUG] System promptè¾ƒé•¿({len(final_system_prompt)} chars),ä½¿ç”¨æ‹†åˆ†ç­–ç•¥:")
            logger.info(f"  - æ‹†åˆ†æˆå¤šä¸ª system message,æ¯ä¸ª < {MAX_SINGLE_MESSAGE_LENGTH} chars")
            logger.info(f"  - é¿å…å•ä¸ª message è¿‡é•¿å¯¼è‡´ç½‘å…³ 503 é”™è¯¯")

            # å°† system prompt æŒ‰æ®µè½æ‹†åˆ†
            # ä¼˜å…ˆåœ¨åˆ†éš”ç¬¦å¤„æ‹†åˆ†: "========================================", "\n\n", "\n"
            parts = []
            current_part = ""
            separators = ["========================================", "\n\n", "\n"]

            # æŒ‰ä¼˜å…ˆçº§å°è¯•æ‹†åˆ†
            for separator in separators:
                if len(final_system_prompt) < MAX_SINGLE_MESSAGE_LENGTH:
                    break

                # å°è¯•æŒ‰æ­¤åˆ†éš”ç¬¦æ‹†åˆ†
                segments = final_system_prompt.split(separator)
                current_part = ""

                for segment in segments:
                    test_part = current_part + separator + segment if current_part else segment

                    if len(test_part) <= MAX_SINGLE_MESSAGE_LENGTH:
                        current_part = test_part
                    else:
                        # å½“å‰éƒ¨åˆ†å·²æ»¡,ä¿å­˜å¹¶å¼€å§‹æ–°éƒ¨åˆ†
                        if current_part:
                            parts.append(current_part.strip())
                        current_part = segment.strip()

                # ä¿å­˜æœ€åçš„éƒ¨åˆ†
                if current_part:
                    parts.append(current_part.strip())

                # å¦‚æœæ‹†åˆ†æˆåŠŸ,é€€å‡ºå¾ªç¯
                if len(parts) > 1 or all(len(p) < MAX_SINGLE_MESSAGE_LENGTH for p in parts):
                    break
                else:
                    # æ‹†åˆ†å¤±è´¥,æ¸…ç©ºé‡è¯•
                    parts = []

            # å¦‚æœæ‹†åˆ†å¤±è´¥,å¼ºåˆ¶æŒ‰å­—ç¬¦é•¿åº¦æ‹†åˆ†
            if not parts:
                logger.warning(f"  - æŒ‰åˆ†éš”ç¬¦æ‹†åˆ†å¤±è´¥,ä½¿ç”¨å¼ºåˆ¶æ‹†åˆ†")
                for i in range(0, len(final_system_prompt), MAX_SINGLE_MESSAGE_LENGTH):
                    parts.append(final_system_prompt[i:i + MAX_SINGLE_MESSAGE_LENGTH])

            logger.info(f"  - æ‹†åˆ†ç»“æœ: {len(parts)} ä¸ª system message")
            for i, part in enumerate(parts):
                logger.info(f"    Part {i+1}: {len(part)} chars")

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨: å¤šä¸ª system message + user message
            messages_for_ai = []
            for part in parts:
                messages_for_ai.append({
                    "role": "system",
                    "content": part
                })

            # æ·»åŠ å¯¹è¯å†å²(å¦‚æœæœ‰)
            # æ³¨æ„: å¦‚æœæœ‰å¯¹è¯å†å²,éœ€è¦ä¿æŒ user-assistant äº¤æ›¿æ ¼å¼
            if len(request.messages) > 1:
                # å°†å†å²æ¶ˆæ¯æ·»åŠ åˆ° system messages ä¹‹å
                for msg in request.messages[:-1]:
                    messages_for_ai.append({
                        "role": msg.role,
                        "content": msg.content
                    })

            # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
            messages_for_ai.append({
                "role": "user",
                "content": user_prompt
            })

        else:
            # System prompté•¿åº¦é€‚ä¸­,ä½¿ç”¨æ ‡å‡†æ ¼å¼(å¸¦ç¼“å­˜)
            logger.info(f"âœ… [DEBUG] System prompté•¿åº¦é€‚ä¸­({len(final_system_prompt)} chars),ä½¿ç”¨æ ‡å‡†æ ¼å¼(å¸¦ç¼“å­˜)")

            if final_system_prompt:
                messages_for_ai.append({
                    "role": "system",
                    "content": final_system_prompt
                })
            messages_for_ai.append({
                "role": "user",
                "content": user_prompt
            })

        # 10. ä½¿ç”¨ AIServiceï¼ˆä¸ admin/ai ä¿æŒä¸€è‡´ï¼Œé¿å…å·®å¼‚ï¼‰
        ai_service = AIService(db)

        # æŸ¥æ‰¾æ¨¡å‹ IDï¼ˆä½¿ç”¨å®é™…çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œè€Œä¸æ˜¯æ•°æ®åº“ä¸»é”®ï¼‰
        model_id_for_ai = llm_model.model_id  # ä½¿ç”¨ model_id å­—æ®µï¼ˆå®é™…çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼‰

        # ğŸ” è°ƒè¯•æ—¥å¿—: æ‰“å°å…³é”®ä¿¡æ¯
        # è®¡ç®—è¯·æ±‚ä½“å¤§å°(ä¼°ç®—)
        import json
        request_body_size = len(json.dumps({"model": model_id_for_ai, "messages": messages_for_ai}).encode('utf-8'))

        logger.info(f"ğŸ“Š [DEBUG] Chat Request Info:")
        logger.info(f"  - Conversation ID: {conversation_id}")
        logger.info(f"  - User ID: {current_user.id}")
        logger.info(f"  - Agent Type: {request.agent_type}")
        logger.info(f"  - Model Type: {agent_model_type}")
        logger.info(f"  - Provider: {llm_model.provider}")
        logger.info(f"  - Model ID for AI: {model_id_for_ai}")
        logger.info(f"  - DB Model: {llm_model.name} (model_id={llm_model.model_id})")
        logger.info(f"  - Base URL: {llm_model.base_url}")
        logger.info(f"  - System Prompt Length: {len(final_system_prompt)} chars")
        logger.info(f"  - User Prompt Length: {len(user_prompt)} chars")
        logger.info(f"  - Messages Count: {len(messages_for_ai)}")
        logger.info(f"  - Estimated Request Body Size: {request_body_size} bytes")
        logger.info(f"  - Temperature: {temperature}, Max Tokens: {max_tokens}")
        logger.info(f"  - Stream: {request.stream}")

        # æ‰“å°å®é™…å‘é€çš„æ¶ˆæ¯ç»“æ„(ç”¨äºè°ƒè¯•503é—®é¢˜)
        logger.info(f"ğŸ“‹ [DEBUG] Messages Structure:")
        for i, msg in enumerate(messages_for_ai):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            content_preview = content[:100] + '...' if len(content) > 100 else content
            # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦
            has_special_chars = any(ord(c) > 127 for c in content)
            logger.info(f"  - Message {i+1}: role={role}, length={len(content)}, has_special_chars={has_special_chars}")
            logger.info(f"    Preview: {content_preview}")

            # å¦‚æœæœ‰ç‰¹æ®Šå­—ç¬¦,æ‰“å°ä¸€äº›ç¤ºä¾‹
            if has_special_chars:
                special_chars = [c for c in content if ord(c) > 127][:10]
                logger.warning(f"    âš ï¸ Special chars found: {special_chars}")

        # æ£€æŸ¥è¯·æ±‚ä½“å¤§å°æ˜¯å¦è¶…è¿‡å®‰å…¨é˜ˆå€¼
        MAX_REQUEST_SIZE = 100000  # 100KB (å¤§å¤šæ•°APIç½‘å…³çš„é™åˆ¶æ˜¯1-10MB)
        if request_body_size > MAX_REQUEST_SIZE:
            logger.warning(f"âš ï¸ [WARNING] Request body size ({request_body_size} bytes) exceeds safe threshold ({MAX_REQUEST_SIZE} bytes)")
            logger.warning(f"  This may cause API gateway 503 errors or timeouts")
            logger.warning(f"  Consider: 1) Reducing system prompt length, 2) Limiting conversation history")
        
        # 11. ç”Ÿæˆå“åº”
        assistant_content = ""  # ç”¨äºåå°ä»»åŠ¡ä¿å­˜

        if request.stream:
            # æµå¼å“åº”
            async def generate_stream():
                nonlocal assistant_content
                try:
                    # é¦–å…ˆå‘é€ conversation_idï¼ˆè®©å‰ç«¯èƒ½å¤Ÿæ›´æ–°ä¼šè¯IDï¼‰
                    yield f"data: {json.dumps({'conversation_id': conversation_id}, ensure_ascii=False)}\n\n"
                    logger.info(f"ğŸ“¤ [DEBUG] Starting stream generation for conversation {conversation_id}")

                    # ä½¿ç”¨ AIService.stream_chatï¼ˆä¸ admin/ai ä¿æŒä¸€è‡´ï¼‰
                    chunk_count = 0
                    async for chunk_json in ai_service.stream_chat(
                        messages=messages_for_ai,
                        model=model_id_for_ai,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        top_p=1.0,
                        frequency_penalty=0.0,
                        presence_penalty=0.0
                    ):
                        chunk_count += 1
                        if chunk_count == 1:
                            logger.info(f"âœ… [DEBUG] Received first chunk from AI service")

                        # AIService.stream_chat è¿”å›çš„æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                        try:
                            chunk_data = json.loads(chunk_json)
                            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                            if "error" in chunk_data:
                                # å¦‚æœæ˜¯é”™è¯¯ï¼Œç›´æ¥ä¼ é€’
                                logger.error(f"âŒ [DEBUG] Received error from AI service: {chunk_data['error']}")
                                yield f"data: {chunk_json}\n\n"
                                return
                            # æå– contentï¼ˆAIService è¿”å›çš„æ ¼å¼ï¼‰
                            delta = chunk_data.get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                assistant_content += content
                                yield f"data: {json.dumps({'content': content}, ensure_ascii=False)}\n\n"
                        except json.JSONDecodeError:
                            # å¦‚æœä¸æ˜¯ JSONï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰ï¼Œç›´æ¥ä½œä¸ºå†…å®¹å¤„ç†
                            assistant_content += chunk_json
                            yield f"data: {json.dumps({'content': chunk_json}, ensure_ascii=False)}\n\n"

                    logger.info(f"âœ… [DEBUG] Stream generation completed. Total chunks: {chunk_count}, Content length: {len(assistant_content)}")
                    yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"

                    # ========== âœ… ç¬¬ä¸‰é˜¶æ®µï¼šç®—åŠ›ç»“ç®—ï¼ˆæçŸ­äº‹åŠ¡ï¼Œ~10msï¼‰ ==========
                    if task_id and freeze_info.get('request_id'):
                        logger.info(f"ğŸ’° [åŸå­ç»“ç®—] å¼€å§‹ç®—åŠ›ç»“ç®—æµç¨‹ï¼Œrequest_id={freeze_info['request_id']}")
                        try:
                            # ä¼°ç®—å®é™…tokenä½¿ç”¨
                            calculator = CoinCalculatorService(db)
                            input_tokens = calculator.estimate_tokens_from_text(user_prompt)
                            output_tokens = calculator.estimate_tokens_from_text(assistant_content)

                            logger.info(f"ğŸ’° [åŸå­ç»“ç®—] Tokenä¼°ç®—å®Œæˆ: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")

                            # è®¡ç®—å®é™…æ¶ˆè€—é‡‘é¢
                            actual_cost = await calculator.calculate_cost(
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                model_id=llm_model.id
                            )

                            logger.info(f"ğŸ’° [åŸå­ç»“ç®—] æˆæœ¬è®¡ç®—å®Œæˆ: {actual_cost}")

                            # âœ… ä½¿ç”¨åŸå­åŒ–ç»“ç®—ï¼ˆç‹¬ç«‹äº‹åŠ¡ï¼Œæ— é”å†²çªï¼‰
                            from db.session import async_session_maker
                            async with async_session_maker() as settle_db:
                                settle_account_service = CoinAccountService(settle_db)
                                settle_result = await settle_account_service.settle_amount_atomic(
                                    user_id=current_user.id,
                                    request_id=freeze_info['request_id'],
                                    actual_cost=actual_cost,
                                    input_tokens=input_tokens,
                                    output_tokens=output_tokens,
                                    model_name=llm_model.name
                                )

                                if settle_result['success']:
                                    logger.info(
                                        f"âœ… [åŸå­ç»“ç®—] ç®—åŠ›ç»“ç®—æˆåŠŸ: "
                                        f"ç”¨æˆ·ID={current_user.id}, "
                                        f"è¾“å…¥Token={input_tokens}, "
                                        f"è¾“å‡ºToken={output_tokens}, "
                                        f"ç»“ç®—é‡‘é¢={actual_cost}"
                                    )
                                else:
                                    logger.error(
                                        f"âŒ [åŸå­ç»“ç®—] ç®—åŠ›ç»“ç®—å¤±è´¥: "
                                        f"ç”¨æˆ·ID={current_user.id}, "
                                        f"é”™è¯¯={settle_result.get('message')}"
                                    )

                        except Exception as e:
                            logger.error(f"âŒ [åŸå­ç»“ç®—] ç®—åŠ›ç»“ç®—å¼‚å¸¸: {str(e)}")
                            import traceback
                            logger.error(f"âŒ [åŸå­ç»“ç®—] ç»“ç®—é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                            # ç»“ç®—å¤±è´¥ä¸å½±å“å¯¹è¯ï¼Œåªè®°å½•é”™è¯¯

                    # æµå¼å®Œæˆåï¼Œè§¦å‘åå°ä»»åŠ¡ä¿å­˜
                    background_tasks.add_task(
                        save_conversation_background_task,
                        conversation_id=conversation_id,
                        user_message=user_prompt,
                        assistant_message=assistant_content,
                        user_tokens=len(user_prompt) // 4,  # ç²—ç•¥ä¼°ç®—tokenæ•°
                        assistant_tokens=len(assistant_content) // 4,
                    )

                except Exception as e:
                    # ğŸ” è¯¦ç»†é”™è¯¯æ—¥å¿—
                    import traceback
                    logger.error(f"âŒ [DEBUG] Stream generation failed:")
                    logger.error(f"  - Error Type: {type(e).__name__}")
                    logger.error(f"  - Error Message: {str(e)}")
                    logger.error(f"  - Conversation ID: {conversation_id}")
                    logger.error(f"  - User ID: {current_user.id}")
                    logger.error(f"  - Model ID: {model_id_for_ai}")
                    logger.error(f"  - Agent Type: {request.agent_type}")
                    logger.error(f"  - System Prompt Length: {len(final_system_prompt)} chars")
                    logger.error(f"  - User Prompt Length: {len(user_prompt)} chars")
                    logger.error(f"  - Temperature: {temperature}, Max Tokens: {max_tokens}")
                    logger.error(f"  - Traceback:\n{traceback.format_exc()}")

                    # ========== âœ… é”™è¯¯æ—¶é€€æ¬¾é¢„å†»ç»“çš„ç®—åŠ›ï¼ˆåŸå­åŒ–é€€æ¬¾ï¼‰ ==========
                    if task_id and freeze_info.get('request_id'):
                        try:
                            from db.session import async_session_maker
                            async with async_session_maker() as refund_db:
                                refund_account_service = CoinAccountService(refund_db)
                                refund_result = await refund_account_service.refund_amount_atomic(
                                    user_id=current_user.id,
                                    request_id=freeze_info['request_id'],
                                    reason="AIç”Ÿæˆå¤±è´¥"
                                )

                                if refund_result['success']:
                                    logger.info(
                                        f"âœ… [åŸå­é€€æ¬¾] é”™è¯¯é€€æ¬¾æˆåŠŸ: "
                                        f"ç”¨æˆ·ID={current_user.id}, request_id={freeze_info['request_id']}"
                                    )
                                else:
                                    logger.error(
                                        f"âŒ [åŸå­é€€æ¬¾] é”™è¯¯é€€æ¬¾å¤±è´¥: "
                                        f"ç”¨æˆ·ID={current_user.id}, "
                                        f"é”™è¯¯={refund_result.get('message')}"
                                    )

                        except Exception as refund_error:
                            logger.error(f"âŒ [åŸå­é€€æ¬¾] é€€æ¬¾å¼‚å¸¸: {str(refund_error)}")

                    error_msg = f"ç”Ÿæˆé”™è¯¯: {str(e)}"
                    yield f"data: {json.dumps({'error': error_msg}, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",
                }
            )
        else:
            # éæµå¼å“åº”
            result = await ai_service.chat(
                messages=messages_for_ai,
                model=model_id_for_ai,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=1.0,
                frequency_penalty=0.0,
                presence_penalty=0.0
            )
            assistant_content = result.get("message", {}).get("content", "")
            
            # ç«‹å³è§¦å‘åå°ä»»åŠ¡ä¿å­˜ï¼ˆä¸é˜»å¡å“åº”ï¼‰
            background_tasks.add_task(
                save_conversation_background_task,
                conversation_id=conversation_id,
                user_message=user_prompt,
                assistant_message=assistant_content,
                user_tokens=len(user_prompt) // 4,
                assistant_tokens=len(assistant_content) // 4,
            )
            
            return ChatResponse(
                success=True,
                content=assistant_content,
                agent_type=request.agent_type,
                model_type=agent_model_type
            )
    
    except (BadRequestException, ServerErrorException):
        raise
    except Exception as e:
        # ğŸ” æ•è·æ‰€æœ‰æœªå¤„ç†çš„å¼‚å¸¸,è®°å½•è¯¦ç»†æ—¥å¿—
        import traceback
        logger.error(f"âŒ [DEBUG] Chat endpoint unexpected error:")
        logger.error(f"  - Error Type: {type(e).__name__}")
        logger.error(f"  - Error Message: {str(e)}")
        logger.error(f"  - User ID: {current_user.id if 'current_user' in locals() else 'N/A'}")
        logger.error(f"  - Model Type: {agent_model_type if 'agent_model_type' in locals() else 'N/A'}")
        logger.error(f"  - Agent Type: {request.agent_type if 'request' in locals() else 'N/A'}")
        logger.error(f"  - Project ID: {request.project_id if 'request' in locals() else 'N/A'}")
        logger.error(f"  - Traceback:\n{traceback.format_exc()}")
        raise ServerErrorException(f"ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.post("/chat/debug")
async def debug_chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_miniprogram_user),
    db: AsyncSession = Depends(get_db)
):
    """
    è°ƒè¯•ç‰ˆchatæ¥å£ - è¿”å›è¯¦ç»†ä¿¡æ¯ä½†ä¸è°ƒç”¨AI

    ç”¨äºæ’æŸ¥503é”™è¯¯,å±•ç¤º:
    1. æ¨¡å‹é…ç½®æŸ¥è¯¢ç»“æœ
    2. æç¤ºè¯æ„å»ºè¿‡ç¨‹å’Œé•¿åº¦
    3. æ¶ˆæ¯æ ¼å¼
    4. ä¸ä¼šå®é™…è°ƒç”¨AI API
    """
    try:
        debug_info = {
            "request_params": {
                "model_type": request.model_type,
                "agent_type": request.agent_type,
                "project_id": request.project_id,
                "stream": request.stream,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
                "messages_count": len(request.messages),
            },
            "step_results": {}
        }

        # 1. éªŒè¯æ¨¡å‹ç±»å‹
        supported_models = LLMFactory.get_supported_models()
        debug_info["step_results"]["model_type_check"] = {
            "is_supported": request.model_type.lower() in supported_models,
            "supported_models": supported_models
        }

        # 2. è·å–æ™ºèƒ½ä½“é…ç½®
        try:
            agent_config = get_agent_config(request.agent_type)
            debug_info["step_results"]["agent_config"] = {
                "found": True,
                "system_prompt_length": len(agent_config.get("system_prompt", "")),
                "temperature": agent_config.get("temperature"),
                "max_tokens": agent_config.get("max_tokens"),
            }
        except Exception as e:
            debug_info["step_results"]["agent_config"] = {
                "found": False,
                "error": str(e)
            }
            # å°è¯•ä»æ•°æ®åº“æŸ¥è¯¢
            try:
                agent_id = int(request.agent_type)
                from sqlalchemy import select
                from models.agent import Agent
                result = await db.execute(
                    select(Agent).where(Agent.id == agent_id)
                )
                db_agent = result.scalar_one_or_none()
                if db_agent:
                    debug_info["step_results"]["agent_config"] = {
                        "found": True,
                        "source": "database",
                        "agent_name": db_agent.name,
                        "system_prompt_length": len(db_agent.system_prompt),
                    }
            except:
                pass

        # 3. æŸ¥è¯¢æ¨¡å‹é…ç½®
        model_type_to_provider = {
            "deepseek": "deepseek",
            "doubao": "doubao",
            "claude": "anthropic"
        }
        provider = model_type_to_provider.get(request.model_type.lower(), request.model_type.lower())
        debug_info["step_results"]["model_query"] = {
            "provider": provider,
            "provider_mapping": model_type_to_provider
        }

        from sqlalchemy import select, and_
        from models.llm_model import LLMModel
        result = await db.execute(
            select(LLMModel).where(
                and_(
                    LLMModel.provider == provider,
                    LLMModel.is_enabled == True
                )
            ).order_by(LLMModel.sort_order).limit(1)
        )
        llm_model = result.scalar_one_or_none()

        if llm_model:
            debug_info["step_results"]["model_query"]["found_model"] = {
                "id": llm_model.id,
                "name": llm_model.name,
                "model_id": llm_model.model_id,
                "provider": llm_model.provider,
                "base_url": llm_model.base_url,
                "has_api_key": bool(llm_model.api_key),
                "api_key_prefix": llm_model.api_key[:10] + "..." if llm_model.api_key else None,
                "is_enabled": llm_model.is_enabled,
            }
        else:
            debug_info["step_results"]["model_query"]["found_model"] = None
            # æŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
            all_enabled = await db.execute(
                select(LLMModel).where(LLMModel.is_enabled == True)
            )
            enabled_models = all_enabled.scalars().all()
            debug_info["step_results"]["model_query"]["all_enabled_models"] = [
                {
                    "id": m.id,
                    "name": m.name,
                    "provider": m.provider,
                    "model_id": m.model_id,
                }
                for m in enabled_models
            ]

        # 4. æ„å»ºæç¤ºè¯(æ¨¡æ‹ŸçœŸå®æµç¨‹ä½†ä¸è°ƒç”¨AI)
        user_prompt = get_latest_user_message(request.messages)
        debug_info["step_results"]["prompt_building"] = {
            "user_prompt_length": len(user_prompt),
            "user_prompt_preview": user_prompt[:200] + "..." if len(user_prompt) > 200 else user_prompt,
        }

        # å¦‚æœæœ‰é¡¹ç›®ID,æ„å»ºIPäººè®¾
        if request.project_id:
            try:
                project_service = ProjectService(db)
                project = await project_service.get_project_by_id(request.project_id, user_id=current_user.id)
                if project:
                    ip_persona_prompt = build_ip_persona_prompt(project)
                    debug_info["step_results"]["prompt_building"]["ip_persona_length"] = len(ip_persona_prompt)
                    debug_info["step_results"]["prompt_building"]["ip_persona_preview"] = ip_persona_prompt[:200] + "..." if len(ip_persona_prompt) > 200 else ip_persona_prompt
            except Exception as e:
                debug_info["step_results"]["prompt_building"]["ip_persona_error"] = str(e)

        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        conversation_context = build_conversation_context(request.messages)
        debug_info["step_results"]["prompt_building"]["conversation_context_length"] = len(conversation_context)

        # æ„å»ºæœ€ç»ˆsystem prompt
        if "agent_config" in debug_info["step_results"] and debug_info["step_results"]["agent_config"].get("found"):
            base_system_prompt = debug_info["step_results"]["agent_config"].get("system_prompt", "")
            # è¿™é‡Œç®€åŒ–å¤„ç†,åªè®¡ç®—é•¿åº¦
            debug_info["step_results"]["prompt_building"]["estimated_system_prompt_length"] = (
                                len(base_system_prompt) +
                                len(conversation_context) +
                                debug_info["step_results"]["prompt_building"].get("ip_persona_length", 0)
            )

        return success(data=debug_info, msg="è°ƒè¯•ä¿¡æ¯è·å–æˆåŠŸ")

    except Exception as e:
        import traceback
        return success(
            data={
                "error": str(e),
                "traceback": traceback.format_exc(),
                "debug_info": debug_info if 'debug_info' in locals() else None
            },
            msg="è°ƒè¯•æ¥å£å‘ç”Ÿé”™è¯¯"
        )


@router.post("/chat/quick")
async def quick_generate(
    content: str = Query(..., description="åˆ›ä½œå†…å®¹/ä¸»é¢˜"),
    agent_type: str = Query(default=AgentType.EFFICIENT_ORAL, description="æ™ºèƒ½ä½“ç±»å‹"),
    project_id: Optional[int] = Query(default=None, description="é¡¹ç›®ID"),
    model_type: str = Query(default="deepseek", description="æ¨¡å‹ç±»å‹"),
    current_user: User = Depends(get_current_miniprogram_user),
    db: AsyncSession = Depends(get_db)
):
    """å¿«é€Ÿåˆ›ä½œæ¥å£ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    request = ChatRequest(
        project_id=project_id,
        agent_type=agent_type,
        messages=[ChatMessage(role="user", content=content)],
        model_type=model_type,
        stream=False
    )
    
    return await generate_chat(request, current_user=current_user, db=db)

