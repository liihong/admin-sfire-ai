"""
Client Content Generation Endpoints
Cç«¯å†…å®¹ç”Ÿæˆæ¥å£ï¼ˆå°ç¨‹åº & PCå®˜ç½‘ï¼‰
æ”¯æŒæ™ºèƒ½ä½“åˆ—è¡¨æŸ¥è¯¢ã€å¯¹è¯å¼å†…å®¹ç”Ÿæˆã€å¿«é€Ÿç”Ÿæˆç­‰åŠŸèƒ½
"""
import json
import uuid
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession

from db import get_db
from models.user import User
from core.deps import get_current_miniprogram_user
from services.project import ProjectService
from services.llm_service import LLMFactory
from services.llm_model import LLMModelService
from services.agent import AgentService
from services.conversation import ConversationService
from services.ai import AIService
from services.coin_account import CoinAccountService
from services.coin_calculator import CoinCalculatorService
from middleware.balance_checker import BalanceCheckerMiddleware
from constants.agent import get_agent_config, get_all_agents, AgentType, AGENT_CONFIGS
from utils.response import success
from utils.exceptions import BadRequestException, ServerErrorException, NotFoundException
from loguru import logger
from core.config import settings

router = APIRouter()


# ============== åå°ä»»åŠ¡å‡½æ•° ==============

async def embed_conversation_background_task(
    conversation_id: int,
    user_message_id: int,
    assistant_message_id: int
):
    """
    åå°ä»»åŠ¡ï¼šå‘é‡åŒ–å¯¹è¯ç‰‡æ®µ

    Args:
        conversation_id: ä¼šè¯ID
        user_message_id: ç”¨æˆ·æ¶ˆæ¯ID
        assistant_message_id: AIå›å¤æ¶ˆæ¯ID
    """
    from db.session import async_session_maker
    from services.conversation import ConversationService

    try:
        async with async_session_maker() as db:
            service = ConversationService(db)
            await service.embed_conversation_async(
                conversation_id=conversation_id,
                user_message_id=user_message_id,
                assistant_message_id=assistant_message_id
            )
            logger.info(f"å‘é‡åŒ–å®Œæˆ: ä¼šè¯{conversation_id}, æ¶ˆæ¯{user_message_id}-{assistant_message_id}")
    except Exception as e:
        logger.error(f"å‘é‡åŒ–å¤±è´¥: ä¼šè¯{conversation_id}, é”™è¯¯: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹


async def save_conversation_background_task(
    conversation_id: int,
    user_message: str,
    assistant_message: str,
    user_tokens: int = 0,
    assistant_tokens: int = 0
):
    """
    åå°ä»»åŠ¡ï¼šä¿å­˜å¯¹è¯æ¶ˆæ¯åˆ°æ•°æ®åº“å¹¶è§¦å‘å‘é‡åŒ–

    Args:
        conversation_id: ä¼šè¯ID
        user_message: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
        assistant_message: AIå›å¤å†…å®¹
        user_tokens: ç”¨æˆ·æ¶ˆæ¯tokenæ•°
        assistant_tokens: AIå›å¤tokenæ•°
    """
    from db.session import async_session_maker
    from services.conversation import ConversationService
    from sqlalchemy import select, desc
    from models.conversation import ConversationMessage

    try:
        # 1. ä¿å­˜å¯¹è¯æ¶ˆæ¯
        async with async_session_maker() as db:
            service = ConversationService(db)
            await service.save_conversation_async(
                conversation_id=conversation_id,
                user_message=user_message,
                assistant_message=assistant_message,
                user_tokens=user_tokens,
                assistant_tokens=assistant_tokens
            )

        # 2. è·å–åˆšä¿å­˜çš„æ¶ˆæ¯IDå¹¶è§¦å‘å‘é‡åŒ–
        async with async_session_maker() as db:
            # æŸ¥è¯¢æœ€æ–°çš„ä¸¤æ¡æ¶ˆæ¯ï¼ˆuser + assistantï¼‰
            query = select(ConversationMessage).where(
                ConversationMessage.conversation_id == conversation_id
            ).order_by(desc(ConversationMessage.sequence)).limit(2)

            result = await db.execute(query)
            messages = list(result.scalars().all())

            if len(messages) == 2:
                # messages[0]æ˜¯assistant, messages[1]æ˜¯userï¼ˆé™åºï¼‰
                assistant_msg = messages[0]
                user_msg = messages[1]

                # è§¦å‘å‘é‡åŒ–ä»»åŠ¡
                await embed_conversation_background_task(
                    conversation_id=conversation_id,
                    user_message_id=user_msg.id,
                    assistant_message_id=assistant_msg.id
                )
            else:
                logger.warning(f"æ— æ³•æ‰¾åˆ°æ¶ˆæ¯è¿›è¡Œå‘é‡åŒ–: ä¼šè¯{conversation_id}")

    except Exception as e:
        logger.error(f"åå°ä»»åŠ¡å¤±è´¥: ä¼šè¯{conversation_id}, é”™è¯¯: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹


# ============== Request/Response Models ==============

class ChatMessage(BaseModel):
    """å¯¹è¯æ¶ˆæ¯æ¨¡å‹"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: 'user' æˆ– 'assistant'")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatRequest(BaseModel):
    """å¯¹è¯å¼åˆ›ä½œè¯·æ±‚æ¨¡å‹"""
    conversation_id: Optional[int] = Field(default=None, description="ä¼šè¯IDï¼ˆå¯é€‰ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°ä¼šè¯ï¼‰")
    project_id: Optional[int] = Field(default=None, description="é¡¹ç›®IDï¼Œç”¨äºè·å–IPäººè®¾ä¿¡æ¯")
    agent_type: str = Field(default=AgentType.EFFICIENT_ORAL, description="æ™ºèƒ½ä½“ç±»å‹")
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯å†å²æ¶ˆæ¯åˆ—è¡¨")
    model_type: Optional[str] = Field(default=None, description="LLMæ¨¡å‹ç±»å‹ï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™ä½¿ç”¨æ™ºèƒ½ä½“é…ç½®çš„æ¨¡å‹ï¼‰")
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0, description="ç”Ÿæˆæ¸©åº¦")
    max_tokens: int = Field(default=2048, ge=1, le=8192, description="æœ€å¤§ç”Ÿæˆtokens")
    stream: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º")


class ChatResponse(BaseModel):
    """å¯¹è¯å“åº”æ¨¡å‹ï¼ˆéæµå¼ï¼‰"""
    success: bool = True
    content: str = Field(..., description="ç”Ÿæˆçš„å†…å®¹")
    agent_type: str = Field(..., description="ä½¿ç”¨çš„æ™ºèƒ½ä½“ç±»å‹")
    model_type: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹ç±»å‹")


class AgentInfo(BaseModel):
    """æ™ºèƒ½ä½“ä¿¡æ¯æ¨¡å‹"""
    type: str  # æ™ºèƒ½ä½“ç±»å‹æ ‡è¯†ï¼Œç”¨äºæ˜ å°„åˆ°åç«¯çš„ agent_type
    id: str  # æ™ºèƒ½ä½“IDï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
    name: str
    icon: str
    description: str


class AgentListResponse(BaseModel):
    """æ™ºèƒ½ä½“åˆ—è¡¨å“åº”"""
    success: bool = True
    agents: List[AgentInfo]


# ============== Helper Functions ==============

def build_ip_persona_prompt(project) -> str:
    """ä»é¡¹ç›®ä¿¡æ¯æ„å»ºIPäººè®¾æç¤ºè¯"""
    if not project:
        return ""
    
    persona = project.get_persona_settings_dict()
    parts = []
    
    parts.append(f"ã€IPä¿¡æ¯ã€‘")
    parts.append(f"- IPåç§°ï¼š{project.name}")
    parts.append(f"- æ‰€å±èµ›é“ï¼š{project.industry}")
    
    if persona.get("introduction"):
        parts.append(f"- IPç®€ä»‹ï¼š{persona['introduction']}")
    
    if persona.get("tone"):
        parts.append(f"- è¯­æ°”é£æ ¼ï¼š{persona['tone']}")
    
    if persona.get("target_audience"):
        parts.append(f"- ç›®æ ‡å—ä¼—ï¼š{persona['target_audience']}")
    
    if persona.get("content_style"):
        parts.append(f"- å†…å®¹é£æ ¼ï¼š{persona['content_style']}")
    
    if persona.get("catchphrase"):
        parts.append(f"- å¸¸ç”¨å£å¤´ç¦…ï¼š{persona['catchphrase']}")
    
    if persona.get("keywords"):
        parts.append(f"- å¸¸ç”¨å…³é”®è¯ï¼š{', '.join(persona['keywords'])}")
    
    if persona.get("taboos"):
        parts.append(f"- å†…å®¹ç¦å¿Œï¼š{', '.join(persona['taboos'])}")
    
    if persona.get("benchmark_accounts"):
        parts.append(f"- å¯¹æ ‡è´¦å·ï¼š{', '.join(persona['benchmark_accounts'])}")
    
    return "\n".join(parts)


def build_final_system_prompt(agent_system_prompt: str, ip_persona_prompt: str) -> str:
    """èåˆæ™ºèƒ½ä½“äººè®¾å’ŒIPç”»åƒï¼Œæ„å»ºæœ€ç»ˆçš„System Prompt"""
    parts = [agent_system_prompt]
    
    if ip_persona_prompt:
        parts.append("\n\n" + "=" * 40)
        parts.append("\nåœ¨åˆ›ä½œæ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹IPäººè®¾è®¾å®šï¼Œç¡®ä¿å†…å®¹ç¬¦åˆè¯¥IPçš„é£æ ¼ç‰¹ç‚¹ï¼š\n")
        parts.append(ip_persona_prompt)
        parts.append("\n" + "=" * 40)
        parts.append("\nè¯·åœ¨ä¿æŒæ™ºèƒ½ä½“ä¸“ä¸šèƒ½åŠ›çš„åŒæ—¶ï¼Œèå…¥ä»¥ä¸ŠIPçš„äººè®¾ç‰¹ç‚¹è¿›è¡Œåˆ›ä½œã€‚")
    
    return "".join(parts)


def format_messages_for_llm(messages: List[ChatMessage]) -> str:
    """å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºç”¨äºLLMçš„prompt"""
    for msg in reversed(messages):
        if msg.role == "user":
            return msg.content
    
    return messages[-1].content if messages else ""


def build_conversation_context(messages: List[ChatMessage]) -> str:
    """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç”¨äºå¤šè½®å¯¹è¯"""
    if len(messages) <= 1:
        return ""
    
    history = messages[:-1]
    if not history:
        return ""
    
    context_parts = ["\nã€å¯¹è¯å†å²ã€‘"]
    for msg in history[-6:]:
        role_name = "ç”¨æˆ·" if msg.role == "user" else "åŠ©æ‰‹"
        context_parts.append(f"{role_name}ï¼š{msg.content}")
    
    context_parts.append("\nè¯·åŸºäºä»¥ä¸Šå¯¹è¯å†å²ï¼Œç»§ç»­å›å¤ç”¨æˆ·çš„æœ€æ–°è¯·æ±‚ã€‚")
    
    return "\n".join(context_parts)


# ============== API Endpoints ==============

@router.get("/agents")
async def list_agents(
    db: AsyncSession = Depends(get_db)
):
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ™ºèƒ½ä½“åˆ—è¡¨ï¼ˆä»æ•°æ®åº“è¯»å–ï¼‰"""
    # ä»æ•°æ®åº“æŸ¥è¯¢å¯ç”¨çš„æ™ºèƒ½ä½“
    from sqlalchemy import select, and_
    from models.agent import Agent
    
    result = await db.execute(
        select(Agent).where(
            Agent.status == 1  # åªè¿”å›ä¸Šæ¶çš„æ™ºèƒ½ä½“
        ).order_by(Agent.sort_order, Agent.created_at)
    )
    db_agents = result.scalars().all()
    
    # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼ï¼Œç¡®ä¿ id ä¸º number ç±»å‹
    agents = []
    for agent in db_agents:
        # å¦‚æœæ•°æ®åº“ä¸­æœ‰ type å­—æ®µï¼Œä½¿ç”¨ agent.typeï¼›å¦åˆ™ä½¿ç”¨ agent.id
        agent_type = str(agent.id)  # æš‚æ—¶ä½¿ç”¨ ID ä½œä¸º type
        
        # å°è¯•ä» config ä¸­è·å– typeï¼ˆå¦‚æœä¹‹å‰æœ‰å­˜å‚¨ï¼‰
        if agent.config and isinstance(agent.config, dict) and "type" in agent.config:
            agent_type = agent.config["type"]
        
        agents.append({
            "type": agent_type,
            "id": agent.id,  # ç»Ÿä¸€ä¸º number ç±»å‹
            "name": agent.name,
            "icon": agent.icon,
            "description": agent.description or ""
        })
    
    return success(data={"agents": agents}, msg="è·å–æˆåŠŸ")


@router.post("/chat")
async def generate_chat(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_miniprogram_user),
    db: AsyncSession = Depends(get_db)
):
    """å¯¹è¯å¼åˆ›ä½œæ¥å£ï¼ˆæ”¯æŒå‘é‡æ£€ç´¢å’Œå¼‚æ­¥ä¿å­˜ï¼‰"""
    try:
        # 0. åˆå§‹åŒ–ä¼šè¯æœåŠ¡
        conversation_service = ConversationService(db)

        # 0.1. è·å–æ™ºèƒ½ä½“é…ç½®å’Œæ¨¡å‹ç±»å‹ï¼ˆæå‰è·å–ï¼Œé¿å…é‡å¤æŸ¥è¯¢ï¼‰
        agent_config = None
        agent_type_source = "preset"
        db_agent = None
        agent_model_type = request.model_type or "doubao"  # é»˜è®¤å€¼

        try:
            agent_config = get_agent_config(request.agent_type)
        except ValueError:
            # å¦‚æœé¢„è®¾é…ç½®ä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¯èƒ½æ˜¯æ•°æ®åº“IDï¼‰
            try:
                agent_id = int(request.agent_type)
                from sqlalchemy import select
                from models.agent import Agent

                result = await db.execute(
                    select(Agent).where(
                        Agent.id == agent_id,
                        Agent.status == 1  # åªæŸ¥è¯¢ä¸Šæ¶çš„æ™ºèƒ½ä½“
                    )
                )
                db_agent = result.scalar_one_or_none()

                if db_agent:
                    # ä»æ•°æ®åº“æ™ºèƒ½ä½“æ„å»ºé…ç½®
                    agent_config = {
                        "system_prompt": db_agent.system_prompt,
                        "temperature": db_agent.config.get("temperature", 0.7) if db_agent.config else 0.7,
                        "max_tokens": db_agent.config.get("max_tokens", 2048) if db_agent.config else 2048,
                    }
                    agent_type_source = "database"
                    # ä½¿ç”¨æ•°æ®åº“æ™ºèƒ½ä½“é…ç½®çš„æ¨¡å‹
                    agent_model_type = db_agent.model
                    logger.info(f"ğŸ“Š [DEBUG] ä½¿ç”¨æ•°æ®åº“æ™ºèƒ½ä½“é…ç½®çš„æ¨¡å‹: {agent_model_type}")
                else:
                    available = ", ".join(AGENT_CONFIGS.keys())
                    raise BadRequestException(f"æ™ºèƒ½ä½“ ID '{agent_id}' ä¸å­˜åœ¨æˆ–å·²ä¸‹æ¶ã€‚å¯ç”¨ç±»å‹: {available}")
            except ValueError:
                # agent_type ä¸æ˜¯æ•°å­—ï¼Œä¹Ÿä¸æ˜¯é¢„è®¾æšä¸¾å€¼
                available = ", ".join(AGENT_CONFIGS.keys())
                raise BadRequestException(f"æœªçŸ¥çš„æ™ºèƒ½ä½“ç±»å‹: '{request.agent_type}'ã€‚å¯ç”¨ç±»å‹: {available}")

        if not agent_config:
            available = ", ".join(AGENT_CONFIGS.keys())
            raise BadRequestException(f"æ— æ³•è·å–æ™ºèƒ½ä½“é…ç½®: '{request.agent_type}'ã€‚å¯ç”¨ç±»å‹: {available}")

        # å¦‚æœæ˜¯é¢„è®¾æ™ºèƒ½ä½“ä¸”æ²¡æœ‰æä¾›model_typeï¼Œä½¿ç”¨é»˜è®¤å€¼
        if agent_type_source == "preset" and not request.model_type:
            agent_model_type = "doubao"
            logger.info(f"ğŸ“Š [DEBUG] ä½¿ç”¨é»˜è®¤æ¨¡å‹: {agent_model_type}")

        # 0.2. ä¸å†éªŒè¯æ¨¡å‹ç±»å‹ï¼Œæ‰€æœ‰æ¨¡å‹ä¿¡æ¯ä»æ•°æ®åº“è¯»å–
        # è¿™æ ·å¯ä»¥æ”¯æŒåŠ¨æ€æ·»åŠ æ–°æ¨¡å‹ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
        logger.info(f"ğŸ“Š [DEBUG] ä½¿ç”¨æ¨¡å‹ç±»å‹: {agent_model_type} (æ¥æº: {agent_type_source})")

        # 0.1. å¤„ç†ä¼šè¯IDï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°ä¼šè¯ï¼‰
        conversation_id = request.conversation_id
        if not conversation_id:
            from schemas.conversation import ConversationCreate
            from sqlalchemy import select
            from models.agent import Agent
            
            # è·å–æ™ºèƒ½ä½“åç§°ç”¨äºç”Ÿæˆä¼šè¯æ ‡é¢˜
            agent_name = "æ–°å¯¹è¯"
            agent_id = None
            if request.agent_type.isdigit():
                agent_id = int(request.agent_type)
                result = await db.execute(
                    select(Agent).where(Agent.id == agent_id)
                )
                db_agent = result.scalar_one_or_none()
                if db_agent:
                    agent_name = db_agent.name
            
            # è·å–ç”¨æˆ·çš„ç¬¬ä¸€å¥è¯ï¼ˆæˆªå–å‰30ä¸ªå­—ç¬¦ï¼‰
            first_message = ""
            for msg in request.messages:
                if msg.role == "user" and msg.content:
                    first_message = msg.content[:30]
                    if len(msg.content) > 30:
                        first_message += "..."
                    break
            
            # ç”Ÿæˆä¼šè¯æ ‡é¢˜ï¼šæ™ºèƒ½ä½“åç§° + ç”¨æˆ·ç¬¬ä¸€å¥è¯
            title = f"{agent_name}: {first_message}" if first_message else agent_name
            
            conversation_data = ConversationCreate(
                agent_id=agent_id,
                project_id=request.project_id,
                model_type=agent_model_type,
                title=title,
            )
            conversation = await conversation_service.create_conversation(
                user_id=current_user.id,
                conversation_data=conversation_data
            )
            conversation_id = conversation.id
        else:
            # éªŒè¯ä¼šè¯æ˜¯å¦å±äºå½“å‰ç”¨æˆ·
            try:
                await conversation_service.get_conversation_by_id(
                    conversation_id=conversation_id,
                    user_id=current_user.id
                )
            except NotFoundException:
                # å¦‚æœä¼šè¯ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ä¼šè¯ï¼ˆå¯èƒ½æ˜¯å‰ç«¯å­˜å‚¨äº†å·²åˆ é™¤çš„ä¼šè¯IDï¼‰
                from schemas.conversation import ConversationCreate
                from sqlalchemy import select
                from models.agent import Agent

                logger.warning(f"ä¼šè¯ {conversation_id} ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»ºæ–°ä¼šè¯ï¼ˆç”¨æˆ·ID: {current_user.id}ï¼‰")
                
                # è·å–æ™ºèƒ½ä½“åç§°ç”¨äºç”Ÿæˆä¼šè¯æ ‡é¢˜
                agent_name = "æ–°å¯¹è¯"
                agent_id = None
                if request.agent_type.isdigit():
                    agent_id = int(request.agent_type)
                    result = await db.execute(
                        select(Agent).where(Agent.id == agent_id)
                    )
                    db_agent = result.scalar_one_or_none()
                    if db_agent:
                        agent_name = db_agent.name
                
                # è·å–ç”¨æˆ·çš„ç¬¬ä¸€å¥è¯ï¼ˆæˆªå–å‰30ä¸ªå­—ç¬¦ï¼‰
                first_message = ""
                for msg in request.messages:
                    if msg.role == "user" and msg.content:
                        first_message = msg.content[:30]
                        if len(msg.content) > 30:
                            first_message += "..."
                        break
                
                # ç”Ÿæˆä¼šè¯æ ‡é¢˜ï¼šæ™ºèƒ½ä½“åç§° + ç”¨æˆ·ç¬¬ä¸€å¥è¯
                title = f"{agent_name}: {first_message}" if first_message else agent_name

                conversation_data = ConversationCreate(
                    agent_id=agent_id,
                    project_id=request.project_id,
                    model_type=agent_model_type,
                    title=title,
                )
                conversation = await conversation_service.create_conversation(
                    user_id=current_user.id,
                    conversation_data=conversation_data
                )
                conversation_id = conversation.id
            except Exception as e:
                # å…¶ä»–é”™è¯¯æ­£å¸¸æŠ›å‡º
                raise

        # 1. è·å–é¡¹ç›®IPç”»åƒï¼ˆå¦‚æœæä¾›äº†project_idï¼‰
        ip_persona_prompt = ""
        if request.project_id:
            project_service = ProjectService(db)
            project = await project_service.get_project_by_id(request.project_id, user_id=current_user.id)
            if project:
                ip_persona_prompt = build_ip_persona_prompt(project)

        # 2. è·å–ç”¨æˆ·æœ€æ–°æ¶ˆæ¯ä½œä¸ºprompt
        user_prompt = format_messages_for_llm(request.messages)
        
        if not user_prompt:
            raise BadRequestException("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # 5. å‘é‡æ£€ç´¢ï¼šæœç´¢ç›¸å…³å†å²ç‰‡æ®µ
        # âš ï¸ ä¸´æ—¶ç¦ç”¨å‘é‡æ£€ç´¢ä»¥æå‡æ€§èƒ½ï¼ˆå¾…åç«¯å‘é‡åŒ–åŠŸèƒ½ä¿®å¤åé‡æ–°å¯ç”¨ï¼‰
        # TODO: ä¿®å¤å‘é‡åŒ–åå°ä»»åŠ¡åé‡æ–°å¯ç”¨æ­¤åŠŸèƒ½
        relevant_chunks = []
        optimized_messages = request.messages  # é»˜è®¤ä½¿ç”¨åŸå§‹æ¶ˆæ¯

        # ç¦ç”¨å‘é‡æ£€ç´¢ä»£ç  - èŠ‚çœ200-500mså“åº”æ—¶é—´
        # try:
        #     # å¯¹ç”¨æˆ·æ–°æ¶ˆæ¯è¿›è¡Œå‘é‡åŒ–å¹¶æœç´¢ç›¸å…³ç‰‡æ®µ
        #     relevant_chunks = await conversation_service.search_relevant_chunks(
        #         conversation_id=conversation_id,
        #         query_text=user_prompt,
        #         top_k=5,
        #         threshold=0.7
        #     )
        #
        #     # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³ç‰‡æ®µï¼Œä½¿ç”¨ä¼˜åŒ–çš„æ¶ˆæ¯ä¸Šä¸‹æ–‡
        #     if relevant_chunks:
        #         optimized_messages = await conversation_service.build_context_from_search(
        #             conversation_id=conversation_id,
        #             query_text=user_prompt,
        #             relevant_chunks=relevant_chunks,
        #             include_recent=2  # åŒ…å«æœ€è¿‘2è½®å¯¹è¯
        #         )
        # except Exception as e:
        #     # å‘é‡æ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ¶ˆæ¯
        #     from loguru import logger
        #     logger.warning(f"å‘é‡æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯: {e}")
        
        # 6. æ„å»ºæœ€ç»ˆSystem Promptï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„æ¶ˆæ¯æˆ–åŸå§‹æ¶ˆæ¯æ„å»ºä¸Šä¸‹æ–‡ï¼‰
        base_system_prompt = agent_config["system_prompt"]
        
        # å¦‚æœæœ‰ä¼˜åŒ–çš„æ¶ˆæ¯ï¼Œæ„å»ºä¸Šä¸‹æ–‡
        if relevant_chunks:
            # ä½¿ç”¨ä¼˜åŒ–åçš„æ¶ˆæ¯æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåªåŒ…å«ç›¸å…³ç‰‡æ®µå’Œæœ€è¿‘æ¶ˆæ¯ï¼‰
            conversation_context = ""
        else:
            # å›é€€åˆ°åŸå§‹é€»è¾‘ï¼šä½¿ç”¨å…¨éƒ¨æ¶ˆæ¯æ„å»ºä¸Šä¸‹æ–‡
            conversation_context = build_conversation_context(optimized_messages)
        
        final_system_prompt = build_final_system_prompt(
            agent_system_prompt=base_system_prompt + conversation_context,
            ip_persona_prompt=ip_persona_prompt,
        )

        # ğŸ” æ£€æŸ¥å¹¶é™åˆ¶ system prompt é•¿åº¦
        MAX_SYSTEM_PROMPT_LENGTH = 8000  # æ ¹æ®æ¨¡å‹é™åˆ¶è°ƒæ•´
        original_length = len(final_system_prompt)
        if original_length > MAX_SYSTEM_PROMPT_LENGTH:
            logger.warning(f"âš ï¸ [DEBUG] System prompt too long ({original_length} chars), truncating to {MAX_SYSTEM_PROMPT_LENGTH} chars")
            logger.warning(f"  - Base agent prompt length: {len(base_system_prompt)} chars")
            logger.warning(f"  - Conversation context length: {len(conversation_context)} chars")
            logger.warning(f"  - IP persona prompt length: {len(ip_persona_prompt)} chars")

            # æ™ºèƒ½æˆªæ–­ç­–ç•¥: ä¿ç•™æ™ºèƒ½ä½“æ ¸å¿ƒprompt,ç²¾ç®€å†å²å’ŒIPä¿¡æ¯
            # 1. å…ˆä¿ç•™å®Œæ•´çš„æ™ºèƒ½ä½“prompt
            truncated_prompt = base_system_prompt

            # 2. å¦‚æœè¿˜æœ‰ç©ºé—´,æ·»åŠ IPäººè®¾çš„å…³é”®éƒ¨åˆ†
            remaining_space = MAX_SYSTEM_PROMPT_LENGTH - len(truncated_prompt) - 100  # ç•™100å­—ç¬¦buffer
            if remaining_space > 0 and ip_persona_prompt:
                # åªä¿ç•™IPäººè®¾çš„å‰Nä¸ªå­—ç¬¦
                ip_persona_truncated = ip_persona_prompt[:remaining_space]
                truncated_prompt = build_final_system_prompt(
                    agent_system_prompt=truncated_prompt,
                    ip_persona_prompt=ip_persona_truncated
                )

            # 3. å¦‚æœè¿˜æ²¡åˆ°é™åˆ¶,æ·»åŠ å¯¹è¯å†å²(æœ€å¤š2è½®)
            if len(truncated_prompt) < MAX_SYSTEM_PROMPT_LENGTH * 0.8 and conversation_context:
                # ç®€åŒ–å¯¹è¯å†å²:åªä¿ç•™æœ€è¿‘2è½®
                simplified_context = "\nã€æœ€è¿‘å¯¹è¯ã€‘" + "\n".join(conversation_context.split("\n")[-6:])
                final_system_prompt = truncated_prompt + "\n\n" + simplified_context
            else:
                final_system_prompt = truncated_prompt

            logger.warning(f"  - After truncation: {len(final_system_prompt)} chars")
        else:
            logger.info(f"âœ… [DEBUG] System prompt length OK: {original_length} chars")

        # å¦‚æœä½¿ç”¨ä¼˜åŒ–åçš„æ¶ˆæ¯ï¼Œéœ€è¦é‡æ–°æ ¼å¼åŒ–user_prompt
        if relevant_chunks:
            # ä»ä¼˜åŒ–çš„æ¶ˆæ¯ä¸­æå–ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€åä¸€æ¡useræ¶ˆæ¯ï¼‰
            user_prompt = user_prompt  # ä¿æŒåŸæ ·ï¼Œå› ä¸ºå·²ç»åœ¨optimized_messagesçš„æœ€å
        
        # 7. ç¡®å®šç”Ÿæˆå‚æ•°
        temperature = request.temperature if request.temperature is not None else agent_config.get("temperature", 0.7)
        max_tokens = request.max_tokens or agent_config.get("max_tokens", 2048)
        
        # 8. ä»ï¿½ï¿½æ®åº“è·å–æ¨¡å‹é…ç½®
        # ç›´æ¥é€šè¿‡ model_type (æˆ– model_id) æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ¨¡å‹é…ç½®
        # æ”¯æŒä¸¤ç§æŸ¥è¯¢æ–¹å¼:
        # 1. é€šè¿‡ provider å­—æ®µæŸ¥è¯¢ (å…¼å®¹æ—§çš„ model_type å¦‚ "deepseek", "doubao")
        # 2. é€šè¿‡ model_id å­—æ®µæŸ¥è¯¢ (æ”¯æŒæ•°æ®åº“ä¸­å­˜å‚¨çš„æ¨¡å‹ID)
        from sqlalchemy import select, and_, or_
        from models.llm_model import LLMModel

        logger.info(f"ğŸ” [DEBUG] Querying model configuration:")
        logger.info(f"  - Requested model_type: {agent_model_type}")

        # å°è¯•é€šè¿‡ provider æˆ– model_id æŸ¥è¯¢
        result = await db.execute(
            select(LLMModel).where(
                and_(
                    or_(
                        LLMModel.provider == agent_model_type.lower(),
                        LLMModel.model_id == agent_model_type,
                        LLMModel.id == int(agent_model_type) if agent_model_type.isdigit() else False
                    ),
                    LLMModel.is_enabled == True
                )
            ).order_by(LLMModel.sort_order).limit(1)
        )
        llm_model = result.scalar_one_or_none()

        if not llm_model:
            # ğŸ” è¯¦ç»†é”™è¯¯æ—¥å¿—: æŸ¥è¯¢å¤±è´¥çš„åŸå› 
            logger.error(f"âŒ [DEBUG] Model not found in database:")
            logger.error(f"  - Requested model_type: {agent_model_type}")

            # æŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹,å¸®åŠ©è°ƒè¯•
            all_enabled = await db.execute(
                select(LLMModel).where(LLMModel.is_enabled == True)
            )
            enabled_models = all_enabled.scalars().all()
            logger.error(f"  - All enabled models in database:")
            for m in enabled_models:
                logger.error(f"    * {m.name} (id={m.id}, provider={m.provider}, model_id={m.model_id}, enabled={m.is_enabled})")

            raise BadRequestException(
                f"æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ '{agent_model_type}'ï¼Œè¯·åœ¨ç®¡ç†åå°é…ç½®æ¨¡å‹"
            )

        logger.info(f"âœ… [DEBUG] Model found: {llm_model.name} (id={llm_model.id}, provider={llm_model.provider})")

        if not llm_model.api_key:
            raise BadRequestException(f"æ¨¡å‹ {llm_model.name} æœªé…ç½® API Keyï¼Œè¯·åœ¨ç®¡ç†åå°é…ç½®")

        # 8.5 ç®—åŠ›é¢„å†»ç»“ï¼ˆåœ¨AIè°ƒç”¨å‰ï¼‰
        task_id = str(uuid.uuid4())
        balance_checker = BalanceCheckerMiddleware(db)
        estimated_output_tokens = request.max_tokens or 2048

        try:
            # è·å–ç”¨æˆ·è¾“å…¥æ–‡æœ¬ç”¨äºä¼°ç®—
            user_input_text = user_prompt  # ä½¿ç”¨ç”¨æˆ·æç¤ºè¯

            freeze_info = await balance_checker.check_and_freeze(
                user_id=current_user.id,
                model_id=llm_model.id,
                input_text=user_input_text,
                task_id=task_id,
                estimated_output_tokens=estimated_output_tokens
            )
            logger.info(f"ğŸ’° [DEBUG] ç®—åŠ›é¢„å†»ç»“æˆåŠŸ: ç”¨æˆ·ID={current_user.id}, é‡‘é¢={freeze_info['frozen_amount']}, ä»»åŠ¡ID={task_id}")
        except BadRequestException as e:
            # ä½™é¢ä¸è¶³ï¼Œç›´æ¥è¿”å›é”™è¯¯
            logger.warning(f"âŒ [DEBUG] ç”¨æˆ·ä½™é¢ä¸è¶³: {str(e)}")
            raise
        except Exception as e:
            # é¢„å†»ç»“å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸é˜»æ­¢è¯·æ±‚ï¼ˆé™çº§å¤„ç†ï¼‰
            logger.warning(f"âš ï¸ [DEBUG] ç®—åŠ›é¢„å†»ç»“å¤±è´¥ï¼ˆé™çº§å¤„ç†ï¼‰: {str(e)}")
            task_id = None  # æ ‡è®°ä¸ºæœªé¢„å†»ç»“ï¼Œè·³è¿‡ç»“ç®—

        # 9. æ„å»º messages åˆ—è¡¨ï¼ˆä¸ AIService å…¼å®¹çš„æ ¼å¼ï¼‰
        # å°† final_system_prompt å’Œ user_prompt è½¬æ¢ä¸º messages æ ¼å¼
        messages_for_ai = []

        # ğŸ” æ™ºèƒ½å¤„ç†é•¿system prompt: ç¡®ä¿å®Œæ•´promptå§‹ç»ˆå‘é€,é¿å…ç½‘å…³503é”™è¯¯
        # ç­–ç•¥: å°†å®Œæ•´system promptèå…¥useræ¶ˆæ¯,é¿å…systemå­—æ®µè¿‡é•¿å¯¼è‡´ç½‘å…³503

        if len(final_system_prompt) > 1500:
            # System promptè¾ƒé•¿,ä½¿ç”¨useræ¶ˆæ¯ç­–ç•¥(é¿å…systemå­—æ®µè¿‡é•¿)
            logger.info(f"ğŸ“Š [DEBUG] System promptè¾ƒé•¿({len(final_system_prompt)} chars),ä½¿ç”¨useræ¶ˆæ¯ç­–ç•¥:")
            logger.info(f"  - å°†system promptèå…¥useræ¶ˆæ¯ä¸­")
            logger.info(f"  - ä¿æŒuser-assistantäº¤æ›¿çš„æ ¼å¼è§„èŒƒ")

            # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡å¯¹è¯
            is_first_message = len(request.messages) <= 2

            if is_first_message:
                # é¦–æ¬¡å¯¹è¯: å°†å®Œæ•´system prompt + ç”¨æˆ·é—®é¢˜ä½œä¸ºuseræ¶ˆæ¯
                logger.info(f"  - é¦–æ¬¡å¯¹è¯: å®Œæ•´prompt({len(final_system_prompt)} chars) + ç”¨æˆ·é—®é¢˜")
                combined_message = f"{final_system_prompt}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{user_prompt}"

                messages_for_ai = [
                    {
                        "role": "user",
                        "content": combined_message
                    }
                ]
            else:
                # åç»­å¯¹è¯: å°†system promptèå…¥å½“å‰useræ¶ˆæ¯,ä¿æŒuser-assistantäº¤æ›¿æ ¼å¼
                logger.info(f"  - åç»­å¯¹è¯: èåˆprompt({len(final_system_prompt)} chars) + å½“å‰é—®é¢˜")
                logger.info(f"  - ä¿æŒuser-assistantäº¤æ›¿çš„æ ¼å¼è§„èŒƒ,é¿å…ç½‘å…³503é”™è¯¯")

                # æŒ‰ç…§user-assistantäº¤æ›¿çš„è§„åˆ™æ„å»ºæ¶ˆæ¯åˆ—è¡¨
                for i, msg in enumerate(request.messages):
                    if msg.role == "user":
                        # åˆ¤æ–­æ˜¯å¦æ˜¯æœ€åä¸€æ¡useræ¶ˆæ¯(å½“å‰é—®é¢˜)
                        is_last_user = True
                        for j in range(i + 1, len(request.messages)):
                            if request.messages[j].role == "user":
                                is_last_user = False
                                break

                        if is_last_user:
                            # æœ€åä¸€æ¡useræ¶ˆæ¯: èåˆsystem prompt
                            enhanced_content = f"{final_system_prompt}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{msg.content}"
                            messages_for_ai.append({
                                "role": "user",
                                "content": enhanced_content
                            })
                        else:
                            # å†å²useræ¶ˆæ¯: ä¿æŒåŸæ ·
                            messages_for_ai.append({
                                "role": msg.role,
                                "content": msg.content
                            })
                    else:
                        # assistantæ¶ˆæ¯: ä¿æŒåŸæ ·
                        messages_for_ai.append({
                            "role": msg.role,
                            "content": msg.content
                        })
        else:
            # System prompté•¿åº¦é€‚ä¸­,ä½¿ç”¨æ ‡å‡†æ ¼å¼(å¸¦ç¼“å­˜)
            logger.info(f"âœ… [DEBUG] System prompté•¿åº¦é€‚ä¸­({len(final_system_prompt)} chars),ä½¿ç”¨æ ‡å‡†æ ¼å¼(å¸¦ç¼“å­˜)")

            if final_system_prompt:
                messages_for_ai.append({
                    "role": "system",
                    "content": final_system_prompt
                })
            messages_for_ai.append({
                "role": "user",
                "content": user_prompt
            })
        
        # 10. ä½¿ç”¨ AIServiceï¼ˆä¸ admin/ai ä¿æŒä¸€è‡´ï¼Œé¿å…å·®å¼‚ï¼‰
        ai_service = AIService(db)

        # æŸ¥æ‰¾æ¨¡å‹ IDï¼ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„æ¨¡å‹ IDï¼‰
        model_id_for_ai = str(llm_model.id)  # ä½¿ç”¨æ•°æ®åº“ ID ä½œä¸ºæ¨¡å‹æ ‡è¯†

        # ğŸ” è°ƒè¯•æ—¥å¿—: æ‰“å°å…³é”®ä¿¡æ¯
        # è®¡ç®—è¯·æ±‚ä½“å¤§å°(ä¼°ç®—)
        import json
        request_body_size = len(json.dumps({"model": model_id_for_ai, "messages": messages_for_ai}).encode('utf-8'))

        logger.info(f"ğŸ“Š [DEBUG] Chat Request Info:")
        logger.info(f"  - Conversation ID: {conversation_id}")
        logger.info(f"  - User ID: {current_user.id}")
        logger.info(f"  - Agent Type: {request.agent_type}")
        logger.info(f"  - Model Type: {agent_model_type}")
        logger.info(f"  - Provider: {llm_model.provider}")
        logger.info(f"  - Model ID for AI: {model_id_for_ai}")
        logger.info(f"  - DB Model: {llm_model.name} (model_id={llm_model.model_id})")
        logger.info(f"  - Base URL: {llm_model.base_url}")
        logger.info(f"  - System Prompt Length: {len(final_system_prompt)} chars")
        logger.info(f"  - User Prompt Length: {len(user_prompt)} chars")
        logger.info(f"  - Messages Count: {len(messages_for_ai)}")
        logger.info(f"  - Estimated Request Body Size: {request_body_size} bytes")
        logger.info(f"  - Temperature: {temperature}, Max Tokens: {max_tokens}")
        logger.info(f"  - Stream: {request.stream}")

        # æ‰“å°å®é™…å‘é€çš„æ¶ˆæ¯ç»“æ„(ç”¨äºè°ƒè¯•503é—®é¢˜)
        logger.info(f"ğŸ“‹ [DEBUG] Messages Structure:")
        for i, msg in enumerate(messages_for_ai):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            content_preview = content[:100] + '...' if len(content) > 100 else content
            # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦
            has_special_chars = any(ord(c) > 127 for c in content)
            logger.info(f"  - Message {i+1}: role={role}, length={len(content)}, has_special_chars={has_special_chars}")
            logger.info(f"    Preview: {content_preview}")

            # å¦‚æœæœ‰ç‰¹æ®Šå­—ç¬¦,æ‰“å°ä¸€äº›ç¤ºä¾‹
            if has_special_chars:
                special_chars = [c for c in content if ord(c) > 127][:10]
                logger.warning(f"    âš ï¸ Special chars found: {special_chars}")

        # æ£€æŸ¥è¯·æ±‚ä½“å¤§å°æ˜¯å¦è¶…è¿‡å®‰å…¨é˜ˆå€¼
        MAX_REQUEST_SIZE = 100000  # 100KB (å¤§å¤šæ•°APIç½‘å…³çš„é™åˆ¶æ˜¯1-10MB)
        if request_body_size > MAX_REQUEST_SIZE:
            logger.warning(f"âš ï¸ [WARNING] Request body size ({request_body_size} bytes) exceeds safe threshold ({MAX_REQUEST_SIZE} bytes)")
            logger.warning(f"  This may cause API gateway 503 errors or timeouts")
            logger.warning(f"  Consider: 1) Reducing system prompt length, 2) Limiting conversation history")
        
        # 11. ç”Ÿæˆå“åº”
        assistant_content = ""  # ç”¨äºåå°ä»»åŠ¡ä¿å­˜

        if request.stream:
            # æµå¼å“åº”
            async def generate_stream():
                nonlocal assistant_content
                try:
                    # é¦–å…ˆå‘é€ conversation_idï¼ˆè®©å‰ç«¯èƒ½å¤Ÿæ›´æ–°ä¼šè¯IDï¼‰
                    yield f"data: {json.dumps({'conversation_id': conversation_id}, ensure_ascii=False)}\n\n"
                    logger.info(f"ğŸ“¤ [DEBUG] Starting stream generation for conversation {conversation_id}")

                    # ä½¿ç”¨ AIService.stream_chatï¼ˆä¸ admin/ai ä¿æŒä¸€è‡´ï¼‰
                    chunk_count = 0
                    async for chunk_json in ai_service.stream_chat(
                        messages=messages_for_ai,
                        model=model_id_for_ai,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        top_p=1.0,
                        frequency_penalty=0.0,
                        presence_penalty=0.0
                    ):
                        chunk_count += 1
                        if chunk_count == 1:
                            logger.info(f"âœ… [DEBUG] Received first chunk from AI service")

                        # AIService.stream_chat è¿”å›çš„æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                        try:
                            chunk_data = json.loads(chunk_json)
                            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                            if "error" in chunk_data:
                                # å¦‚æœæ˜¯é”™è¯¯ï¼Œç›´æ¥ä¼ é€’
                                logger.error(f"âŒ [DEBUG] Received error from AI service: {chunk_data['error']}")
                                yield f"data: {chunk_json}\n\n"
                                return
                            # æå– contentï¼ˆAIService è¿”å›çš„æ ¼å¼ï¼‰
                            delta = chunk_data.get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                assistant_content += content
                                yield f"data: {json.dumps({'content': content}, ensure_ascii=False)}\n\n"
                        except json.JSONDecodeError:
                            # å¦‚æœä¸æ˜¯ JSONï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰ï¼Œç›´æ¥ä½œä¸ºå†…å®¹å¤„ç†
                            assistant_content += chunk_json
                            yield f"data: {json.dumps({'content': chunk_json}, ensure_ascii=False)}\n\n"

                    logger.info(f"âœ… [DEBUG] Stream generation completed. Total chunks: {chunk_count}, Content length: {len(assistant_content)}")
                    yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"

                    # 8.6 ç®—åŠ›ç»“ç®—ï¼ˆåœ¨AIè°ƒç”¨åï¼‰
                    if task_id:
                        logger.info(f"ğŸ’° [DEBUG] å¼€å§‹ç®—åŠ›ç»“ç®—æµç¨‹ï¼Œtask_id={task_id}")
                        try:
                            # ä¼°ç®—å®é™…tokenä½¿ç”¨
                            calculator = CoinCalculatorService(db)
                            input_tokens = calculator.estimate_tokens_from_text(user_prompt)
                            output_tokens = calculator.estimate_tokens_from_text(assistant_content)

                            logger.info(f"ğŸ’° [DEBUG] Tokenä¼°ç®—å®Œæˆ: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")

                            # è®¡ç®—å®é™…æ¶ˆè€—é‡‘é¢ï¼ˆæ³¨æ„å‚æ•°é¡ºåºï¼šinput_tokens, output_tokens, model_idï¼‰
                            actual_cost = await calculator.calculate_cost(
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                model_id=llm_model.id
                            )

                            logger.info(f"ğŸ’° [DEBUG] æˆæœ¬è®¡ç®—å®Œæˆ: {actual_cost} (ç±»å‹: {type(actual_cost)})")

                            # æ‰§è¡Œç»“ç®—
                            await balance_checker.settle(
                                user_id=current_user.id,
                                task_id=task_id,
                                actual_cost=actual_cost,
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                model_id=llm_model.id,
                                is_error=False,
                                error_code=None
                            )
                            logger.info(f"ğŸ’° [DEBUG] ç®—åŠ›ç»“ç®—æˆåŠŸ: ç”¨æˆ·ID={current_user.id}, è¾“å…¥Token={input_tokens}, è¾“å‡ºToken={output_tokens}, ç»“ç®—é‡‘é¢={actual_cost}")
                        except Exception as e:
                            logger.error(f"âŒ [DEBUG] ç®—åŠ›ç»“ç®—å¤±è´¥: {str(e)}")
                            import traceback
                            logger.error(f"âŒ [DEBUG] ç»“ç®—é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                            # ç»“ç®—å¤±è´¥ä¸å½±å“å¯¹è¯ï¼Œåªè®°å½•é”™è¯¯

                    # æµå¼å®Œæˆåï¼Œè§¦å‘åå°ä»»åŠ¡ä¿å­˜
                    background_tasks.add_task(
                        save_conversation_background_task,
                        conversation_id=conversation_id,
                        user_message=user_prompt,
                        assistant_message=assistant_content,
                        user_tokens=len(user_prompt) // 4,  # ç²—ç•¥ä¼°ç®—tokenæ•°
                        assistant_tokens=len(assistant_content) // 4,
                    )

                except Exception as e:
                    # ğŸ” è¯¦ç»†é”™è¯¯æ—¥å¿—
                    import traceback
                    logger.error(f"âŒ [DEBUG] Stream generation failed:")
                    logger.error(f"  - Error Type: {type(e).__name__}")
                    logger.error(f"  - Error Message: {str(e)}")
                    logger.error(f"  - Conversation ID: {conversation_id}")
                    logger.error(f"  - User ID: {current_user.id}")
                    logger.error(f"  - Model ID: {model_id_for_ai}")
                    logger.error(f"  - Agent Type: {request.agent_type}")
                    logger.error(f"  - System Prompt Length: {len(final_system_prompt)} chars")
                    logger.error(f"  - User Prompt Length: {len(user_prompt)} chars")
                    logger.error(f"  - Temperature: {temperature}, Max Tokens: {max_tokens}")
                    logger.error(f"  - Traceback:\n{traceback.format_exc()}")

                    # 8.7 é”™è¯¯æ—¶é€€æ¬¾é¢„å†»ç»“çš„ç®—åŠ›
                    if task_id:
                        try:
                            await balance_checker.settle(
                                user_id=current_user.id,
                                task_id=task_id,
                                actual_cost=0,  # é”™è¯¯æ—¶å®é™…æ¶ˆè€—ä¸º0
                                input_tokens=0,
                                output_tokens=0,
                                model_id=llm_model.id,
                                is_error=True,
                                error_code="generation_error"
                            )
                            logger.info(f"ğŸ’° [DEBUG] é”™è¯¯é€€æ¬¾æˆåŠŸ: ç”¨æˆ·ID={current_user.id}, ä»»åŠ¡ID={task_id}")
                        except Exception as refund_error:
                            logger.error(f"âŒ [DEBUG] é”™è¯¯é€€æ¬¾å¤±è´¥: {str(refund_error)}")

                    error_msg = f"ç”Ÿæˆé”™è¯¯: {str(e)}"
                    yield f"data: {json.dumps({'error': error_msg}, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",
                }
            )
        else:
            # éæµå¼å“åº”
            result = await ai_service.chat(
                messages=messages_for_ai,
                model=model_id_for_ai,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=1.0,
                frequency_penalty=0.0,
                presence_penalty=0.0
            )
            assistant_content = result.get("message", {}).get("content", "")
            
            # ç«‹å³è§¦å‘åå°ä»»åŠ¡ä¿å­˜ï¼ˆä¸é˜»å¡å“åº”ï¼‰
            background_tasks.add_task(
                save_conversation_background_task,
                conversation_id=conversation_id,
                user_message=user_prompt,
                assistant_message=assistant_content,
                user_tokens=len(user_prompt) // 4,
                assistant_tokens=len(assistant_content) // 4,
            )
            
            return ChatResponse(
                success=True,
                content=assistant_content,
                agent_type=request.agent_type,
                model_type=agent_model_type
            )
    
    except (BadRequestException, ServerErrorException):
        raise
    except Exception as e:
        # ğŸ” æ•è·æ‰€æœ‰æœªå¤„ç†çš„å¼‚å¸¸,è®°å½•è¯¦ç»†æ—¥å¿—
        import traceback
        logger.error(f"âŒ [DEBUG] Chat endpoint unexpected error:")
        logger.error(f"  - Error Type: {type(e).__name__}")
        logger.error(f"  - Error Message: {str(e)}")
        logger.error(f"  - User ID: {current_user.id if 'current_user' in locals() else 'N/A'}")
        logger.error(f"  - Model Type: {agent_model_type if 'agent_model_type' in locals() else 'N/A'}")
        logger.error(f"  - Agent Type: {request.agent_type if 'request' in locals() else 'N/A'}")
        logger.error(f"  - Project ID: {request.project_id if 'request' in locals() else 'N/A'}")
        logger.error(f"  - Traceback:\n{traceback.format_exc()}")
        raise ServerErrorException(f"ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.post("/chat/debug")
async def debug_chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_miniprogram_user),
    db: AsyncSession = Depends(get_db)
):
    """
    è°ƒè¯•ç‰ˆchatæ¥å£ - è¿”å›è¯¦ç»†ä¿¡æ¯ä½†ä¸è°ƒç”¨AI

    ç”¨äºæ’æŸ¥503é”™è¯¯,å±•ç¤º:
    1. æ¨¡å‹é…ç½®æŸ¥è¯¢ç»“æœ
    2. æç¤ºè¯æ„å»ºè¿‡ç¨‹å’Œé•¿åº¦
    3. æ¶ˆæ¯æ ¼å¼
    4. ä¸ä¼šå®é™…è°ƒç”¨AI API
    """
    try:
        debug_info = {
            "request_params": {
                "model_type": request.model_type,
                "agent_type": request.agent_type,
                "project_id": request.project_id,
                "stream": request.stream,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
                "messages_count": len(request.messages),
            },
            "step_results": {}
        }

        # 1. éªŒè¯æ¨¡å‹ç±»å‹
        supported_models = LLMFactory.get_supported_models()
        debug_info["step_results"]["model_type_check"] = {
            "is_supported": request.model_type.lower() in supported_models,
            "supported_models": supported_models
        }

        # 2. è·å–æ™ºèƒ½ä½“é…ç½®
        try:
            agent_config = get_agent_config(request.agent_type)
            debug_info["step_results"]["agent_config"] = {
                "found": True,
                "system_prompt_length": len(agent_config.get("system_prompt", "")),
                "temperature": agent_config.get("temperature"),
                "max_tokens": agent_config.get("max_tokens"),
            }
        except Exception as e:
            debug_info["step_results"]["agent_config"] = {
                "found": False,
                "error": str(e)
            }
            # å°è¯•ä»æ•°æ®åº“æŸ¥è¯¢
            try:
                agent_id = int(request.agent_type)
                from sqlalchemy import select
                from models.agent import Agent
                result = await db.execute(
                    select(Agent).where(Agent.id == agent_id)
                )
                db_agent = result.scalar_one_or_none()
                if db_agent:
                    debug_info["step_results"]["agent_config"] = {
                        "found": True,
                        "source": "database",
                        "agent_name": db_agent.name,
                        "system_prompt_length": len(db_agent.system_prompt),
                    }
            except:
                pass

        # 3. æŸ¥è¯¢æ¨¡å‹é…ç½®
        model_type_to_provider = {
            "deepseek": "deepseek",
            "doubao": "doubao",
            "claude": "anthropic"
        }
        provider = model_type_to_provider.get(request.model_type.lower(), request.model_type.lower())
        debug_info["step_results"]["model_query"] = {
            "provider": provider,
            "provider_mapping": model_type_to_provider
        }

        from sqlalchemy import select, and_
        from models.llm_model import LLMModel
        result = await db.execute(
            select(LLMModel).where(
                and_(
                    LLMModel.provider == provider,
                    LLMModel.is_enabled == True
                )
            ).order_by(LLMModel.sort_order).limit(1)
        )
        llm_model = result.scalar_one_or_none()

        if llm_model:
            debug_info["step_results"]["model_query"]["found_model"] = {
                "id": llm_model.id,
                "name": llm_model.name,
                "model_id": llm_model.model_id,
                "provider": llm_model.provider,
                "base_url": llm_model.base_url,
                "has_api_key": bool(llm_model.api_key),
                "api_key_prefix": llm_model.api_key[:10] + "..." if llm_model.api_key else None,
                "is_enabled": llm_model.is_enabled,
            }
        else:
            debug_info["step_results"]["model_query"]["found_model"] = None
            # æŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
            all_enabled = await db.execute(
                select(LLMModel).where(LLMModel.is_enabled == True)
            )
            enabled_models = all_enabled.scalars().all()
            debug_info["step_results"]["model_query"]["all_enabled_models"] = [
                {
                    "id": m.id,
                    "name": m.name,
                    "provider": m.provider,
                    "model_id": m.model_id,
                }
                for m in enabled_models
            ]

        # 4. æ„å»ºæç¤ºè¯(æ¨¡æ‹ŸçœŸå®æµç¨‹ä½†ä¸è°ƒç”¨AI)
        user_prompt = format_messages_for_llm(request.messages)
        debug_info["step_results"]["prompt_building"] = {
            "user_prompt_length": len(user_prompt),
            "user_prompt_preview": user_prompt[:200] + "..." if len(user_prompt) > 200 else user_prompt,
        }

        # å¦‚æœæœ‰é¡¹ç›®ID,æ„å»ºIPäººè®¾
        if request.project_id:
            try:
                project_service = ProjectService(db)
                project = await project_service.get_project_by_id(request.project_id, user_id=current_user.id)
                if project:
                    ip_persona_prompt = build_ip_persona_prompt(project)
                    debug_info["step_results"]["prompt_building"]["ip_persona_length"] = len(ip_persona_prompt)
                    debug_info["step_results"]["prompt_building"]["ip_persona_preview"] = ip_persona_prompt[:200] + "..." if len(ip_persona_prompt) > 200 else ip_persona_prompt
            except Exception as e:
                debug_info["step_results"]["prompt_building"]["ip_persona_error"] = str(e)

        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        conversation_context = build_conversation_context(request.messages)
        debug_info["step_results"]["prompt_building"]["conversation_context_length"] = len(conversation_context)

        # æ„å»ºæœ€ç»ˆsystem prompt
        if "agent_config" in debug_info["step_results"] and debug_info["step_results"]["agent_config"].get("found"):
            base_system_prompt = debug_info["step_results"]["agent_config"].get("system_prompt", "")
            # è¿™é‡Œç®€åŒ–å¤„ç†,åªè®¡ç®—é•¿åº¦
            debug_info["step_results"]["prompt_building"]["estimated_system_prompt_length"] = (
                                len(base_system_prompt) +
                                len(conversation_context) +
                                debug_info["step_results"]["prompt_building"].get("ip_persona_length", 0)
            )

        return success(data=debug_info, msg="è°ƒè¯•ä¿¡æ¯è·å–æˆåŠŸ")

    except Exception as e:
        import traceback
        return success(
            data={
                "error": str(e),
                "traceback": traceback.format_exc(),
                "debug_info": debug_info if 'debug_info' in locals() else None
            },
            msg="è°ƒè¯•æ¥å£å‘ç”Ÿé”™è¯¯"
        )


@router.post("/chat/quick")
async def quick_generate(
    content: str = Query(..., description="åˆ›ä½œå†…å®¹/ä¸»é¢˜"),
    agent_type: str = Query(default=AgentType.EFFICIENT_ORAL, description="æ™ºèƒ½ä½“ç±»å‹"),
    project_id: Optional[int] = Query(default=None, description="é¡¹ç›®ID"),
    model_type: str = Query(default="deepseek", description="æ¨¡å‹ç±»å‹"),
    current_user: User = Depends(get_current_miniprogram_user),
    db: AsyncSession = Depends(get_db)
):
    """å¿«é€Ÿåˆ›ä½œæ¥å£ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    request = ChatRequest(
        project_id=project_id,
        agent_type=agent_type,
        messages=[ChatMessage(role="user", content=content)],
        model_type=model_type,
        stream=False
    )
    
    return await generate_chat(request, current_user=current_user, db=db)

