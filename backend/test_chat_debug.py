"""
æµ‹è¯•èŠå¤©æ¥å£çš„è°ƒè¯•è„šæœ¬
ç”¨äºè¯Šæ–­ä¸ºä»€ä¹ˆåªè¿”å› conversation_id è€Œæ²¡æœ‰è¿”å›å¯¹è¯å†…å®¹
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import select
from db.session import async_session_maker
from models.agent import Agent
from models.llm_model import LLMModel


async def debug_chat():
    """è°ƒè¯•èŠå¤©æµç¨‹"""
    print("=" * 60)
    print("èŠå¤©æ¥å£è°ƒè¯•")
    print("=" * 60)

    async with async_session_maker() as db:
        # 1. æŸ¥è¯¢æ™ºèƒ½ä½“8
        print("\n[1] æŸ¥è¯¢æ™ºèƒ½ä½“8...")
        result = await db.execute(select(Agent).where(Agent.id == 8))
        agent = result.scalar_one_or_none()

        if not agent:
            print("âŒ æ™ºèƒ½ä½“8ä¸å­˜åœ¨")
            return

        print(f"âœ… æ‰¾åˆ°æ™ºèƒ½ä½“: {agent.name}")
        print(f"   - ID: {agent.id}")
        print(f"   - modelå­—æ®µ: {agent.model}")
        print(f"   - status: {agent.status}")
        print(f"   - system_prompté•¿åº¦: {len(agent.system_prompt)}")

        # 2. æŸ¥è¯¢æ¨¡å‹é…ç½®
        print(f"\n[2] æŸ¥è¯¢æ¨¡å‹é…ç½® (agent.model = '{agent.model}')...")

        # å°è¯•ä¸‰ç§æ–¹å¼æŸ¥è¯¢æ¨¡å‹
        from sqlalchemy import and_, or_

        result = await db.execute(
            select(LLMModel).where(
                and_(
                    or_(
                        LLMModel.provider == agent.model.lower(),
                        LLMModel.model_id == agent.model,
                        LLMModel.id == int(agent.model) if agent.model.isdigit() else False
                    ),
                    LLMModel.is_enabled == True
                )
            ).order_by(LLMModel.sort_order).limit(1)
        )
        llm_model = result.scalar_one_or_none()

        if not llm_model:
            print(f"âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ '{agent.model}'")
            print("\næ‰€æœ‰å¯ç”¨çš„æ¨¡å‹:")
            result = await db.execute(select(LLMModel).where(LLMModel.is_enabled == True))
            models = result.scalars().all()
            for m in models:
                print(f"   - {m.name} (id={m.id}, provider={m.provider}, model_id={m.model_id})")
            return

        print(f"âœ… æ‰¾åˆ°æ¨¡å‹é…ç½®:")
        print(f"   - ID: {llm_model.id}")
        print(f"   - Name: {llm_model.name}")
        print(f"   - Provider: {llm_model.provider}")
        print(f"   - Model ID: {llm_model.model_id}")
        print(f"   - Base URL: {llm_model.base_url}")
        print(f"   - API Key: {'å·²é…ç½®' if llm_model.api_key else 'âŒæœªé…ç½®'}")
        print(f"   - Enabled: {llm_model.is_enabled}")

        # 3. æ£€æŸ¥ API Key æ ¼å¼
        if llm_model.api_key:
            key_preview = llm_model.api_key[:10] + "..." if len(llm_model.api_key) > 10 else llm_model.api_key
            print(f"   - API Key Preview: {key_preview}")

        # 4. æµ‹è¯• API è°ƒç”¨
        print(f"\n[3] æµ‹è¯•è°ƒç”¨ AI æœåŠ¡...")
        try:
            from services.content import AIService

            ai_service = AIService(db)

            # æ„å»ºæµ‹è¯•æ¶ˆæ¯
            test_messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹"},
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·å›å¤'æµ‹è¯•æˆåŠŸ'"}
            ]

            print(f"   - Model ID for AI: {llm_model.id}")
            print(f"   - Messages: {len(test_messages)} æ¡")

            # æµ‹è¯•æµå¼è°ƒç”¨
            print(f"\n   å¼€å§‹æµå¼è°ƒç”¨...")
            chunk_count = 0
            async for chunk_json in ai_service.stream_chat(
                messages=test_messages,
                model=str(llm_model.id),
                temperature=0.7,
                max_tokens=100,
            ):
                chunk_count += 1
                import json
                try:
                    chunk_data = json.loads(chunk_json)
                    if "error" in chunk_data:
                        print(f"   âŒ æ”¶åˆ°é”™è¯¯: {chunk_data['error']}")
                        break
                    if "delta" in chunk_data:
                        delta = chunk_data.get("delta", {})
                        content = delta.get("content", "")
                        if content:
                            print(f"   âœ… æ”¶åˆ°å†…å®¹å— [{chunk_count}]: {content[:20]}...")
                except:
                    print(f"   ğŸ“¦ æ”¶åˆ°åŸå§‹å—: {chunk_json[:100]}...")

            if chunk_count > 0:
                print(f"\nâœ… æµ‹è¯•æˆåŠŸ! å…±æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®å—")
            else:
                print(f"\nâŒ æµ‹è¯•å¤±è´¥! æ²¡æœ‰æ”¶åˆ°ä»»ä½•æ•°æ®å—")

        except Exception as e:
            print(f"âŒ AI æœåŠ¡è°ƒç”¨å¤±è´¥:")
            print(f"   - é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"   - é”™è¯¯ä¿¡æ¯: {str(e)}")
            import traceback
            print(f"   - Traceback:\n{traceback.format_exc()}")


if __name__ == "__main__":
    asyncio.run(debug_chat())
